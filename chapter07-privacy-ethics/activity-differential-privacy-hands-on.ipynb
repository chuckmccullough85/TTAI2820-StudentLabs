{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29566012",
   "metadata": {},
   "source": [
    "# Chapter 7 Activity: Differential Privacy in Action\n",
    "\n",
    "**Estimated Time: 30 minutes**\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this activity, you will be able to:\n",
    "- Implement basic differential privacy mechanisms\n",
    "- Analyze privacy-utility trade-offs in real datasets\n",
    "- Compare different noise mechanisms for privacy protection\n",
    "- Evaluate privacy budget consumption and composition\n",
    "\n",
    "## Scenario: Privacy-Preserving Healthcare Analytics\n",
    "\n",
    "You are a data scientist at MedSecure Analytics, tasked with analyzing patient data while ensuring strict privacy protection. The hospital wants to publish statistics about patient demographics and treatment outcomes, but must comply with HIPAA and implement differential privacy to protect individual patient information.\n",
    "\n",
    "Your mission: Implement differential privacy mechanisms and analyze the trade-offs between privacy protection and data utility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525b41bf",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Generate Synthetic Patient Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec501010",
   "metadata": {},
   "source": [
    "### What We're About to Do\n",
    "\n",
    "In this first section, we'll import the essential Python libraries needed for differential privacy implementation. We'll also create a synthetic healthcare dataset that mimics real patient data but is completely safe to experiment with. This includes patient demographics, medical conditions, and treatment information.\n",
    "\n",
    "The synthetic data generation is important because it allows us to practice differential privacy techniques without any real privacy concerns while learning the concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93032f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"SETUP: Differential Privacy Laboratory Initialized\")\n",
    "print(\"STATUS: Privacy protection mechanisms ready\")\n",
    "print(\"READY: Synthetic patient data generator prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6805a9",
   "metadata": {},
   "source": [
    "### Understanding the Output\n",
    "\n",
    "The output above confirms that all necessary libraries are loaded and ready. We've imported:\n",
    "- **NumPy & Pandas**: For data manipulation and mathematical operations\n",
    "- **Matplotlib & Seaborn**: For creating visualizations of privacy-utility trade-offs\n",
    "- **SciPy**: For statistical functions\n",
    "- **Warnings**: Suppressed to keep output clean during experimentation\n",
    "\n",
    "The random seed ensures that everyone running this notebook gets identical results, making it easier to follow along and compare outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2676a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic patient dataset\n",
    "def generate_patient_data(n_patients=1000):\n",
    "    \"\"\"Generate synthetic patient data for privacy analysis\"\"\"\n",
    "    \n",
    "    # Patient demographics\n",
    "    ages = np.random.normal(45, 15, n_patients).astype(int)\n",
    "    ages = np.clip(ages, 18, 90)  # Realistic age range\n",
    "    \n",
    "    genders = np.random.choice(['M', 'F'], n_patients, p=[0.48, 0.52])\n",
    "    \n",
    "    # Medical conditions (binary indicators)\n",
    "    diabetes = np.random.binomial(1, 0.12, n_patients)\n",
    "    hypertension = np.random.binomial(1, 0.25, n_patients)\n",
    "    heart_disease = np.random.binomial(1, 0.08, n_patients)\n",
    "    \n",
    "    # Treatment costs (influenced by conditions)\n",
    "    base_cost = np.random.exponential(5000, n_patients)\n",
    "    condition_multiplier = 1 + 0.5 * diabetes + 0.3 * hypertension + 0.8 * heart_disease\n",
    "    treatment_costs = base_cost * condition_multiplier\n",
    "    \n",
    "    # Hospital stay length\n",
    "    stay_length = np.random.poisson(3, n_patients) + 1\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'patient_id': range(1, n_patients + 1),\n",
    "        'age': ages,\n",
    "        'gender': genders,\n",
    "        'diabetes': diabetes,\n",
    "        'hypertension': hypertension,\n",
    "        'heart_disease': heart_disease,\n",
    "        'treatment_cost': treatment_costs,\n",
    "        'stay_length': stay_length\n",
    "    })\n",
    "\n",
    "# Generate the dataset\n",
    "patient_data = generate_patient_data(1000)\n",
    "print(f\"GENERATED: Dataset with {len(patient_data)} patients\")\n",
    "print(\"\\nDATASET OVERVIEW:\")\n",
    "print(patient_data.describe())\n",
    "patient_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4649dd",
   "metadata": {},
   "source": [
    "### Understanding the Synthetic Dataset\n",
    "\n",
    "The output shows our synthetic dataset contains realistic healthcare data:\n",
    "- **1,000 patients** with ages ranging from 18-90 years\n",
    "- **Medical conditions**: Diabetes (12%), Hypertension (25%), Heart Disease (8%) - these percentages match real population statistics\n",
    "- **Treatment costs**: Influenced by medical conditions, with more complex cases costing more\n",
    "- **Hospital stays**: Typically 1-7 days with most patients staying 3-4 days\n",
    "\n",
    "This synthetic data is perfect for learning differential privacy because:\n",
    "1. It mimics real healthcare patterns without using actual patient data\n",
    "2. We know the \"ground truth\" to measure privacy-utility trade-offs\n",
    "3. We can experiment freely without privacy concerns\n",
    "\n",
    "Notice how the data shows realistic relationships - patients with diabetes or heart disease have higher treatment costs, demonstrating the complex correlations found in real medical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a471a818",
   "metadata": {},
   "source": [
    "## Part 1: Implementing Basic Differential Privacy Mechanisms (10 minutes)\n",
    "\n",
    "Let's implement the core differential privacy mechanisms: Laplace and Gaussian noise addition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d971ae7b",
   "metadata": {},
   "source": [
    "### What Are Differential Privacy Mechanisms?\n",
    "\n",
    "Before we implement the mechanisms, let's understand what we're building:\n",
    "\n",
    "**Differential Privacy** protects individual privacy by adding carefully calibrated noise to query results. The key insight is that by adding random noise, we can hide whether any specific individual was in the dataset while still providing useful aggregate statistics.\n",
    "\n",
    "**Two Main Mechanisms:**\n",
    "1. **Laplace Mechanism**: Adds noise from a Laplace distribution - provides \"pure\" differential privacy\n",
    "2. **Gaussian Mechanism**: Adds noise from a Gaussian (normal) distribution - provides \"approximate\" differential privacy with slightly different guarantees\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Epsilon (ε)**: The privacy parameter - smaller values mean stronger privacy but more noise\n",
    "- **Sensitivity**: How much a single person's data can change the query result\n",
    "- **Privacy Budget**: Like spending money - each query \"costs\" some privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3e9b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferentialPrivacyMechanisms:\n",
    "    \"\"\"Implementation of core differential privacy mechanisms\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.privacy_budget_used = 0.0\n",
    "        self.query_history = []\n",
    "    \n",
    "    def laplace_mechanism(self, true_value, sensitivity, epsilon):\n",
    "        \"\"\"Add Laplace noise for pure differential privacy\"\"\"\n",
    "        scale = sensitivity / epsilon\n",
    "        noise = np.random.laplace(0, scale)\n",
    "        \n",
    "        # Update privacy accounting\n",
    "        self.privacy_budget_used += epsilon\n",
    "        self.query_history.append({\n",
    "            'mechanism': 'Laplace',\n",
    "            'epsilon': epsilon,\n",
    "            'sensitivity': sensitivity,\n",
    "            'noise_scale': scale\n",
    "        })\n",
    "        \n",
    "        return true_value + noise\n",
    "    \n",
    "    def gaussian_mechanism(self, true_value, sensitivity, epsilon, delta=1e-5):\n",
    "        \"\"\"Add Gaussian noise for (ε,δ)-differential privacy\"\"\"\n",
    "        # Calculate noise scale for Gaussian mechanism\n",
    "        scale = sensitivity * np.sqrt(2 * np.log(1.25 / delta)) / epsilon\n",
    "        noise = np.random.normal(0, scale)\n",
    "        \n",
    "        # Update privacy accounting\n",
    "        self.privacy_budget_used += epsilon\n",
    "        self.query_history.append({\n",
    "            'mechanism': 'Gaussian',\n",
    "            'epsilon': epsilon,\n",
    "            'delta': delta,\n",
    "            'sensitivity': sensitivity,\n",
    "            'noise_scale': scale\n",
    "        })\n",
    "        \n",
    "        return true_value + noise\n",
    "    \n",
    "    def private_count(self, data, epsilon):\n",
    "        \"\"\"Count query with differential privacy\"\"\"\n",
    "        true_count = len(data)\n",
    "        sensitivity = 1  # Adding/removing one person changes count by 1\n",
    "        return self.laplace_mechanism(true_count, sensitivity, epsilon)\n",
    "    \n",
    "    def private_mean(self, data, data_range, epsilon):\n",
    "        \"\"\"Mean query with differential privacy\"\"\"\n",
    "        # Clamp data to known range\n",
    "        clamped_data = np.clip(data, data_range[0], data_range[1])\n",
    "        true_mean = np.mean(clamped_data)\n",
    "        \n",
    "        # Sensitivity calculation for bounded mean\n",
    "        sensitivity = (data_range[1] - data_range[0]) / len(data)\n",
    "        \n",
    "        return self.laplace_mechanism(true_mean, sensitivity, epsilon)\n",
    "    \n",
    "    def private_histogram(self, data, bins, epsilon):\n",
    "        \"\"\"Histogram query with differential privacy\"\"\"\n",
    "        # Create true histogram\n",
    "        hist, bin_edges = np.histogram(data, bins=bins)\n",
    "        \n",
    "        # Add noise to each bin (sensitivity = 1 for histogram)\n",
    "        sensitivity = 1\n",
    "        epsilon_per_bin = epsilon / len(hist)  # Split privacy budget\n",
    "        \n",
    "        private_hist = []\n",
    "        for count in hist:\n",
    "            noisy_count = self.laplace_mechanism(count, sensitivity, epsilon_per_bin)\n",
    "            private_hist.append(max(0, noisy_count))  # Ensure non-negative\n",
    "        \n",
    "        return np.array(private_hist), bin_edges\n",
    "    \n",
    "    def get_privacy_report(self):\n",
    "        \"\"\"Generate privacy budget usage report\"\"\"\n",
    "        return {\n",
    "            'total_epsilon_used': self.privacy_budget_used,\n",
    "            'queries_executed': len(self.query_history),\n",
    "            'query_details': self.query_history\n",
    "        }\n",
    "\n",
    "# Initialize our differential privacy toolkit\n",
    "dp = DifferentialPrivacyMechanisms()\n",
    "print(\"INITIALIZED: Differential Privacy mechanisms initialized\")\n",
    "print(\"READY: Ready to execute private queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ae11e3",
   "metadata": {},
   "source": [
    "### Understanding the Implementation\n",
    "\n",
    "We've just created a comprehensive differential privacy toolkit! Here's what each component does:\n",
    "\n",
    "**Core Mechanisms:**\n",
    "- `laplace_mechanism()`: Adds Laplace-distributed noise for pure ε-differential privacy\n",
    "- `gaussian_mechanism()`: Adds Gaussian noise for (ε,δ)-differential privacy\n",
    "\n",
    "**Query Types:**\n",
    "- `private_count()`: Counts records with noise (sensitivity = 1 because adding/removing one person changes the count by exactly 1)\n",
    "- `private_mean()`: Calculates averages with privacy protection by first clamping data to known ranges\n",
    "- `private_histogram()`: Creates histograms where each bin gets independent noise\n",
    "\n",
    "**Privacy Accounting:**\n",
    "The class tracks every query executed and the privacy budget consumed. This is crucial because:\n",
    "1. Privacy \"compounds\" - multiple queries reveal more information\n",
    "2. We need to stay within our total privacy budget\n",
    "3. Regulatory compliance often requires detailed privacy auditing\n",
    "\n",
    "The implementation handles the complex mathematics behind differential privacy while providing a simple interface for data analysts to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8c4041",
   "metadata": {},
   "source": [
    "### Exercise 1: Basic Private Queries\n",
    "\n",
    "Let's execute some basic private queries on our patient dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b2e25d",
   "metadata": {},
   "source": [
    "### Getting Ready for Your First Private Queries\n",
    "\n",
    "Now we'll execute three basic queries to see differential privacy in action. Each query uses ε = 0.1, which provides strong privacy protection but will add noticeable noise to the results.\n",
    "\n",
    "We'll compare the true (non-private) results with the private results to see the privacy-utility trade-off in practice. Remember: the noise is random, so your results will be slightly different each time you run this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c32f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Execute basic private queries\n",
    "print(\"EXERCISE 1: Basic Private Queries\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Query 1: Total number of patients\n",
    "true_patient_count = len(patient_data)\n",
    "private_patient_count = dp.private_count(patient_data, epsilon=0.1)\n",
    "\n",
    "print(f\"PATIENT COUNT:\")\n",
    "print(f\"   True count: {true_patient_count}\")\n",
    "print(f\"   Private count: {private_patient_count:.1f}\")\n",
    "print(f\"   Error: {abs(true_patient_count - private_patient_count):.1f}\")\n",
    "\n",
    "# Query 2: Average age\n",
    "true_avg_age = patient_data['age'].mean()\n",
    "private_avg_age = dp.private_mean(patient_data['age'], data_range=[18, 90], epsilon=0.1)\n",
    "\n",
    "print(f\"\\nAVERAGE AGE:\")\n",
    "print(f\"   True average: {true_avg_age:.2f} years\")\n",
    "print(f\"   Private average: {private_avg_age:.2f} years\")\n",
    "print(f\"   Error: {abs(true_avg_age - private_avg_age):.2f} years\")\n",
    "\n",
    "# Query 3: Number of diabetes patients\n",
    "true_diabetes_count = patient_data['diabetes'].sum()\n",
    "private_diabetes_count = dp.private_count(patient_data[patient_data['diabetes'] == 1], epsilon=0.1)\n",
    "\n",
    "print(f\"\\nDIABETES PATIENTS:\")\n",
    "print(f\"   True count: {true_diabetes_count}\")\n",
    "print(f\"   Private count: {private_diabetes_count:.1f}\")\n",
    "print(f\"   Error: {abs(true_diabetes_count - private_diabetes_count):.1f}\")\n",
    "\n",
    "# Check privacy budget usage\n",
    "print(f\"\\nPRIVACY BUDGET USED: ε = {dp.privacy_budget_used:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87b5f99",
   "metadata": {},
   "source": [
    "### Analyzing Your First Private Query Results\n",
    "\n",
    "Look at the results above - this is differential privacy in action! Notice several important patterns:\n",
    "\n",
    "**Error Analysis:**\n",
    "- Each query shows some error between the true and private results\n",
    "- The error varies because we're adding random noise for privacy protection\n",
    "- With ε = 0.1 (strong privacy), we expect errors of roughly 10-50 for count queries\n",
    "\n",
    "**Privacy Budget Consumption:**\n",
    "- We've used ε = 0.3 total (0.1 for each of the three queries)\n",
    "- This \"composition\" means our privacy guarantees are now ε = 0.3 overall\n",
    "- Each additional query costs more privacy budget\n",
    "\n",
    "**Key Insights:**\n",
    "- **Patient Count**: Small error is acceptable for aggregate statistics\n",
    "- **Average Age**: A few years of error might be tolerable depending on the use case\n",
    "- **Diabetes Count**: Medical researchers might find this level of error reasonable for population studies\n",
    "\n",
    "The fundamental trade-off is clear: more privacy protection (lower ε) means higher errors, but better protection for individual patients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655edc35",
   "metadata": {},
   "source": [
    "## Part 2: Privacy-Utility Trade-off Analysis (10 minutes)\n",
    "\n",
    "Now let's analyze how different privacy parameters (epsilon values) affect the accuracy of our results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f98a458",
   "metadata": {},
   "source": [
    "### Why Study Privacy-Utility Trade-offs?\n",
    "\n",
    "The next analysis will systematically explore how different privacy levels (ε values) affect query accuracy. This is crucial for real-world applications because:\n",
    "\n",
    "**Business Decisions:** Organizations need to balance privacy protection with data utility for decision-making\n",
    "**Regulatory Compliance:** Understanding the trade-offs helps choose appropriate privacy parameters for different types of sensitive data\n",
    "**Risk Assessment:** Different queries may require different privacy levels based on sensitivity\n",
    "\n",
    "We'll test ε values from 0.01 (very strong privacy) to 10.0 (weak privacy) and measure the average error across multiple trials. This gives us a scientific understanding of the privacy-utility relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a09a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_privacy_utility_tradeoff():\n",
    "    \"\"\"Analyze the trade-off between privacy and utility\"\"\"\n",
    "    \n",
    "    # Test different epsilon values from very private to less private\n",
    "    epsilon_values = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "    \n",
    "    # True values for comparison\n",
    "    true_count = len(patient_data)\n",
    "    true_avg_age = patient_data['age'].mean()\n",
    "    true_avg_cost = patient_data['treatment_cost'].mean()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for epsilon in epsilon_values:\n",
    "        # Run multiple trials to get average error\n",
    "        trials = 20\n",
    "        count_errors = []\n",
    "        age_errors = []\n",
    "        cost_errors = []\n",
    "        \n",
    "        for _ in range(trials):\n",
    "            # Fresh DP instance for each trial\n",
    "            dp_trial = DifferentialPrivacyMechanisms()\n",
    "            \n",
    "            # Execute private queries\n",
    "            private_count = dp_trial.private_count(patient_data, epsilon)\n",
    "            private_avg_age = dp_trial.private_mean(patient_data['age'], [18, 90], epsilon)\n",
    "            private_avg_cost = dp_trial.private_mean(patient_data['treatment_cost'], [0, 50000], epsilon)\n",
    "            \n",
    "            # Calculate absolute errors\n",
    "            count_errors.append(abs(true_count - private_count))\n",
    "            age_errors.append(abs(true_avg_age - private_avg_age))\n",
    "            cost_errors.append(abs(true_avg_cost - private_avg_cost))\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'epsilon': epsilon,\n",
    "            'count_error': np.mean(count_errors),\n",
    "            'age_error': np.mean(age_errors),\n",
    "            'cost_error': np.mean(cost_errors),\n",
    "            'count_std': np.std(count_errors),\n",
    "            'age_std': np.std(age_errors),\n",
    "            'cost_std': np.std(cost_errors)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Perform trade-off analysis\n",
    "print(\"INITIALIZING: Analyzing Privacy-Utility Trade-offs...\")\n",
    "tradeoff_results = analyze_privacy_utility_tradeoff()\n",
    "\n",
    "print(\"\\nRESULTS: Privacy-Utility Trade-off Results:\")\n",
    "print(tradeoff_results.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720b24fb",
   "metadata": {},
   "source": [
    "### Understanding the Trade-off Results\n",
    "\n",
    "The table above shows how query accuracy changes with different privacy levels:\n",
    "\n",
    "**Key Patterns:**\n",
    "- **Lower ε (e.g., 0.01)**: Very high errors but maximum privacy protection\n",
    "- **Higher ε (e.g., 10.0)**: Much lower errors but weaker privacy protection\n",
    "- **Standard deviation**: Shows the variability in errors across multiple trials\n",
    "\n",
    "**Real-World Interpretation:**\n",
    "- **ε = 0.1**: Suitable for highly sensitive data where privacy is paramount\n",
    "- **ε = 1.0**: A common choice balancing privacy and utility for many applications\n",
    "- **ε = 5.0+**: Appropriate when some privacy protection is needed but accuracy is critical\n",
    "\n",
    "Notice how different query types have different error sensitivities. Treatment costs show higher absolute errors because the values are larger (thousands of dollars vs. years of age)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c6302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the privacy-utility trade-off\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Count Error vs Epsilon\n",
    "axes[0].errorbar(tradeoff_results['epsilon'], tradeoff_results['count_error'], \n",
    "                yerr=tradeoff_results['count_std'], marker='o', capsize=5)\n",
    "axes[0].set_xlabel('Privacy Parameter (ε)')\n",
    "axes[0].set_ylabel('Average Count Error')\n",
    "axes[0].set_title('Patient Count Query Error')\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Age Error vs Epsilon\n",
    "axes[1].errorbar(tradeoff_results['epsilon'], tradeoff_results['age_error'], \n",
    "                yerr=tradeoff_results['age_std'], marker='s', capsize=5, color='orange')\n",
    "axes[1].set_xlabel('Privacy Parameter (ε)')\n",
    "axes[1].set_ylabel('Average Age Error (years)')\n",
    "axes[1].set_title('Average Age Query Error')\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Cost Error vs Epsilon\n",
    "axes[2].errorbar(tradeoff_results['epsilon'], tradeoff_results['cost_error'], \n",
    "                yerr=tradeoff_results['cost_std'], marker='^', capsize=5, color='green')\n",
    "axes[2].set_xlabel('Privacy Parameter (ε)')\n",
    "axes[2].set_ylabel('Average Cost Error ($)')\n",
    "axes[2].set_title('Treatment Cost Query Error')\n",
    "axes[2].set_xscale('log')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"KEY OBSERVATIONS:\")\n",
    "print(\"• Lower ε (more privacy) → Higher query error\")\n",
    "print(\"• Higher ε (less privacy) → Lower query error\")\n",
    "print(\"• Error decreases exponentially as ε increases\")\n",
    "print(\"• Different queries have different error sensitivities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78162e7",
   "metadata": {},
   "source": [
    "### Interpreting the Privacy-Utility Visualizations\n",
    "\n",
    "The three charts above reveal the fundamental privacy-utility relationship:\n",
    "\n",
    "**Chart Analysis:**\n",
    "1. **Left (Count Error)**: Shows patient count queries have relatively small absolute errors even with strong privacy\n",
    "2. **Middle (Age Error)**: Age queries show moderate errors - a few years difference is often acceptable for population studies\n",
    "3. **Right (Cost Error)**: Treatment cost queries show the highest absolute errors due to the large value range ($0-$50,000)\n",
    "\n",
    "**Key Business Insights:**\n",
    "- **Logarithmic Scale**: The x-axis uses a log scale because the relationship is exponential - small increases in ε dramatically reduce error\n",
    "- **Error Bars**: Show the uncertainty/variability in results - important for setting expectations with stakeholders\n",
    "- **Sweet Spot**: Many organizations find ε ≈ 1.0 provides a good balance for most use cases\n",
    "\n",
    "**Decision Framework:**\n",
    "- High-stakes decisions may need ε ≥ 2.0 for acceptable accuracy\n",
    "- Population research might work well with ε = 0.5-1.0  \n",
    "- Public datasets with strong privacy requirements might use ε ≤ 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3743fc24",
   "metadata": {},
   "source": [
    "## Part 3: Comparing Noise Mechanisms (5 minutes)\n",
    "\n",
    "Let's compare the Laplace and Gaussian mechanisms for differential privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05364d04",
   "metadata": {},
   "source": [
    "### Laplace vs. Gaussian: Choosing Your Noise Mechanism\n",
    "\n",
    "Different noise mechanisms provide different privacy guarantees and have different mathematical properties. Understanding when to use each is important for real-world implementations.\n",
    "\n",
    "**Laplace Mechanism:**\n",
    "- Provides \"pure\" ε-differential privacy\n",
    "- Has heavier tails (more extreme values possible)\n",
    "- Simpler mathematical analysis\n",
    "- Standard choice for most applications\n",
    "\n",
    "**Gaussian Mechanism:**\n",
    "- Provides (ε,δ)-differential privacy (slightly different guarantee)\n",
    "- Has lighter tails (extreme values less likely)\n",
    "- Better composition properties for multiple queries\n",
    "- Often preferred for complex query sequences\n",
    "\n",
    "We'll compare both mechanisms using the same privacy parameter to see their practical differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de026363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_noise_mechanisms(epsilon=1.0, n_trials=1000):\n",
    "    \"\"\"Compare Laplace vs Gaussian noise mechanisms\"\"\"\n",
    "    \n",
    "    true_value = 100.0  # The value we want to privatize\n",
    "    sensitivity = 1.0   # How much one person can change the result\n",
    "    delta = 1e-5       # Small probability parameter for Gaussian\n",
    "    \n",
    "    laplace_results = []\n",
    "    gaussian_results = []\n",
    "    \n",
    "    for _ in range(n_trials):\n",
    "        # Laplace mechanism\n",
    "        laplace_scale = sensitivity / epsilon\n",
    "        laplace_noise = np.random.laplace(0, laplace_scale)\n",
    "        laplace_results.append(true_value + laplace_noise)\n",
    "        \n",
    "        # Gaussian mechanism  \n",
    "        gaussian_scale = sensitivity * np.sqrt(2 * np.log(1.25 / delta)) / epsilon\n",
    "        gaussian_noise = np.random.normal(0, gaussian_scale)\n",
    "        gaussian_results.append(true_value + gaussian_noise)\n",
    "    \n",
    "    return np.array(laplace_results), np.array(gaussian_results)\n",
    "\n",
    "# Compare the two mechanisms\n",
    "print(\"COMPARING: Noise Mechanisms\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "laplace_vals, gaussian_vals = compare_noise_mechanisms(epsilon=1.0)\n",
    "true_value = 100.0\n",
    "\n",
    "# Calculate error statistics\n",
    "laplace_error = np.abs(laplace_vals - true_value)\n",
    "gaussian_error = np.abs(gaussian_vals - true_value)\n",
    "\n",
    "print(f\"Mechanism Comparison (ε = 1.0):\")\n",
    "print(f\"\\nLaplace Mechanism:\")\n",
    "print(f\"   Mean error: {np.mean(laplace_error):.2f}\")\n",
    "print(f\"   Std error: {np.std(laplace_error):.2f}\")\n",
    "print(f\"   95th percentile error: {np.percentile(laplace_error, 95):.2f}\")\n",
    "\n",
    "print(f\"\\nGaussian Mechanism:\")\n",
    "print(f\"   Mean error: {np.mean(gaussian_error):.2f}\")\n",
    "print(f\"   Std error: {np.std(gaussian_error):.2f}\")\n",
    "print(f\"   95th percentile error: {np.percentile(gaussian_error, 95):.2f}\")\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(laplace_vals, bins=50, alpha=0.7, label='Laplace', density=True)\n",
    "plt.hist(gaussian_vals, bins=50, alpha=0.7, label='Gaussian', density=True)\n",
    "plt.axvline(true_value, color='red', linestyle='--', label='True Value')\n",
    "plt.xlabel('Privatized Value')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Privatized Values')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(laplace_error, bins=50, alpha=0.7, label='Laplace Error', density=True)\n",
    "plt.hist(gaussian_error, bins=50, alpha=0.7, label='Gaussian Error', density=True)\n",
    "plt.xlabel('Absolute Error')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Errors')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKEY DIFFERENCES:\")\n",
    "print(\"• Laplace: Pure ε-DP, heavier tails, simpler analysis\")\n",
    "print(\"• Gaussian: (ε,δ)-DP, lighter tails, better composition properties\")\n",
    "print(\"• Choice depends on privacy requirements and composition needs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78d56c5",
   "metadata": {},
   "source": [
    "### Understanding the Mechanism Comparison Results\n",
    "\n",
    "The analysis above reveals important practical differences between the two mechanisms:\n",
    "\n",
    "**Statistical Differences:**\n",
    "- **Mean Error**: Both mechanisms typically show similar average error magnitudes\n",
    "- **Error Distribution**: Laplace has heavier tails (more extreme outliers), while Gaussian has a more concentrated distribution\n",
    "- **95th Percentile**: Shows the worst-case errors you might expect in practice\n",
    "\n",
    "**Practical Implications:**\n",
    "- **Laplace**: Better for applications where you want to avoid complex δ parameters and prefer simpler guarantees\n",
    "- **Gaussian**: Better when you're running many queries and need good composition properties\n",
    "- **Consistency**: Gaussian tends to produce more consistent results (lower variability)\n",
    "\n",
    "**Visualization Insights:**\n",
    "- **Left Chart**: Shows the full distribution of privatized values around the true value (100)\n",
    "- **Right Chart**: Focuses on error magnitudes - notice how Gaussian has a more peaked distribution while Laplace has longer tails\n",
    "\n",
    "**Choosing Between Them:**\n",
    "- Use **Laplace** for simple, one-off queries or when regulations require pure ε-DP\n",
    "- Use **Gaussian** for complex analytics workflows with multiple related queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87b5cc",
   "metadata": {},
   "source": [
    "## Part 4: Privacy Budget Management (5 minutes)\n",
    "\n",
    "Let's explore how to manage privacy budgets and understand composition effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1ab85",
   "metadata": {},
   "source": [
    "### Why Privacy Budget Management Matters\n",
    "\n",
    "In real-world applications, organizations rarely run just one query. They need ongoing analytics while maintaining privacy protection. This creates a critical challenge: **privacy budget composition**.\n",
    "\n",
    "**The Problem:**\n",
    "- Each query consumes some privacy budget (ε)\n",
    "- Multiple queries compound the privacy loss\n",
    "- Without tracking, you might accidentally exceed safe privacy levels\n",
    "\n",
    "**The Solution:**\n",
    "We'll implement an advanced privacy accountant that:\n",
    "- Tracks every query and its privacy cost\n",
    "- Prevents budget overspending\n",
    "- Provides detailed audit trails for compliance\n",
    "- Visualizes budget consumption over time\n",
    "\n",
    "This mirrors real-world systems where privacy engineers must carefully manage privacy budgets across entire organizations or research projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c69d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedPrivacyAccountant:\n",
    "    \"\"\"Advanced privacy budget accounting with composition\"\"\"\n",
    "    \n",
    "    def __init__(self, total_budget=1.0):\n",
    "        self.total_budget = total_budget\n",
    "        self.remaining_budget = total_budget\n",
    "        self.query_log = []\n",
    "    \n",
    "    def execute_query(self, query_name, epsilon_cost, query_function, *args, **kwargs):\n",
    "        \"\"\"Execute a query with privacy budget checking\"\"\"\n",
    "        \n",
    "        if epsilon_cost > self.remaining_budget:\n",
    "            raise ValueError(f\"Insufficient privacy budget! Need {epsilon_cost}, have {self.remaining_budget}\")\n",
    "        \n",
    "        # Execute the query\n",
    "        result = query_function(*args, **kwargs)\n",
    "        \n",
    "        # Update budget\n",
    "        self.remaining_budget -= epsilon_cost\n",
    "        \n",
    "        # Log the query\n",
    "        self.query_log.append({\n",
    "            'query_name': query_name,\n",
    "            'epsilon_cost': epsilon_cost,\n",
    "            'remaining_budget': self.remaining_budget,\n",
    "            'timestamp': len(self.query_log) + 1\n",
    "        })\n",
    "        \n",
    "        print(f\"EXECUTED: Query '{query_name}' executed (ε={epsilon_cost})\")\n",
    "        print(f\"   Remaining budget: {self.remaining_budget:.3f}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_budget_report(self):\n",
    "        \"\"\"Generate comprehensive budget report\"\"\"\n",
    "        used_budget = self.total_budget - self.remaining_budget\n",
    "        \n",
    "        print(\"\\nREPORT: Privacy Budget Report\")\n",
    "        print(\"=\" * 30)\n",
    "        print(f\"Total budget: {self.total_budget}\")\n",
    "        print(f\"Used budget: {used_budget:.3f} ({used_budget/self.total_budget*100:.1f}%)\")\n",
    "        print(f\"Remaining budget: {self.remaining_budget:.3f}\")\n",
    "        print(f\"Queries executed: {len(self.query_log)}\")\n",
    "        \n",
    "        if self.query_log:\n",
    "            print(\"\\nHISTORY: Query History:\")\n",
    "            for query in self.query_log:\n",
    "                print(f\"  {query['timestamp']}. {query['query_name']} (ε={query['epsilon_cost']})\")\n",
    "    \n",
    "    def visualize_budget_usage(self):\n",
    "        \"\"\"Visualize privacy budget consumption over time\"\"\"\n",
    "        if not self.query_log:\n",
    "            print(\"No queries executed yet!\")\n",
    "            return\n",
    "        \n",
    "        # Calculate cumulative budget usage\n",
    "        timestamps = [0] + [q['timestamp'] for q in self.query_log]\n",
    "        remaining = [self.total_budget] + [q['remaining_budget'] for q in self.query_log]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(timestamps, remaining, marker='o', linewidth=2, markersize=6)\n",
    "        plt.axhline(y=0, color='red', linestyle='--', alpha=0.7, label='Budget Exhausted')\n",
    "        plt.xlabel('Query Number')\n",
    "        plt.ylabel('Remaining Privacy Budget (ε)')\n",
    "        plt.title('Privacy Budget Consumption Over Time')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Add query labels\n",
    "        for i, query in enumerate(self.query_log):\n",
    "            plt.annotate(query['query_name'], \n",
    "                        (query['timestamp'], query['remaining_budget']),\n",
    "                        xytext=(5, 5), textcoords='offset points',\n",
    "                        fontsize=8, alpha=0.8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize privacy accountant with budget\n",
    "accountant = AdvancedPrivacyAccountant(total_budget=1.0)\n",
    "print(\"INITIALIZED: Privacy Budget Manager initialized with ε = 1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcdf640",
   "metadata": {},
   "source": [
    "### Understanding the Privacy Accountant\n",
    "\n",
    "We've just implemented a sophisticated privacy budget management system! Here's what it provides:\n",
    "\n",
    "**Core Features:**\n",
    "- **Budget Checking**: Prevents queries that would exceed the remaining privacy budget\n",
    "- **Query Logging**: Maintains a complete audit trail of all privacy-consuming operations\n",
    "- **Real-time Tracking**: Shows remaining budget after each query\n",
    "- **Compliance Reporting**: Generates detailed reports for regulatory requirements\n",
    "\n",
    "**Key Methods:**\n",
    "- `execute_query()`: Safely runs queries while checking budget constraints\n",
    "- `get_budget_report()`: Provides comprehensive budget usage statistics  \n",
    "- `visualize_budget_usage()`: Creates charts showing budget consumption over time\n",
    "\n",
    "**Real-World Applications:**\n",
    "- **Healthcare**: Tracking privacy budget across multiple research studies\n",
    "- **Finance**: Managing privacy across different analytical teams\n",
    "- **Government**: Ensuring public dataset releases stay within privacy limits\n",
    "- **Technology**: Balancing user privacy with product analytics\n",
    "\n",
    "This accountant would be integrated into production data systems to provide automatic privacy protection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75839d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Execute a series of queries while managing budget\n",
    "print(\"Exercise: Privacy Budget Management\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create a fresh DP mechanism for budget-tracked queries\n",
    "dp_managed = DifferentialPrivacyMechanisms()\n",
    "\n",
    "try:\n",
    "    # Query 1: Patient count (ε = 0.2)\n",
    "    result1 = accountant.execute_query(\n",
    "        \"Patient Count\", 0.2,\n",
    "        dp_managed.private_count, patient_data, 0.2\n",
    "    )\n",
    "    \n",
    "    # Query 2: Average age (ε = 0.3)  \n",
    "    result2 = accountant.execute_query(\n",
    "        \"Average Age\", 0.3,\n",
    "        dp_managed.private_mean, patient_data['age'], [18, 90], 0.3\n",
    "    )\n",
    "    \n",
    "    # Query 3: Diabetes prevalence (ε = 0.2)\n",
    "    diabetes_patients = patient_data[patient_data['diabetes'] == 1]\n",
    "    result3 = accountant.execute_query(\n",
    "        \"Diabetes Count\", 0.2,\n",
    "        dp_managed.private_count, diabetes_patients, 0.2\n",
    "    )\n",
    "    \n",
    "    # Query 4: Age histogram (ε = 0.25)\n",
    "    result4 = accountant.execute_query(\n",
    "        \"Age Histogram\", 0.25,\n",
    "        dp_managed.private_histogram, patient_data['age'], 10, 0.25\n",
    "    )\n",
    "    \n",
    "    # Try one more query that might exceed budget\n",
    "    print(\"\\nAttempting query that might exceed budget...\")\n",
    "    result5 = accountant.execute_query(\n",
    "        \"Treatment Cost Average\", 0.3,\n",
    "        dp_managed.private_mean, patient_data['treatment_cost'], [0, 50000], 0.3\n",
    "    )\n",
    "    \n",
    "except ValueError as e:\n",
    "    print(f\"Budget exceeded: {e}\")\n",
    "\n",
    "# Generate budget report\n",
    "accountant.get_budget_report()\n",
    "\n",
    "# Visualize budget usage\n",
    "print(\"\\nPrivacy Budget Usage Visualization:\")\n",
    "accountant.visualize_budget_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c139e2",
   "metadata": {},
   "source": [
    "### Analyzing the Budget Management Results\n",
    "\n",
    "The exercise above demonstrates several critical aspects of privacy budget management:\n",
    "\n",
    "**What Just Happened:**\n",
    "- **Successful Queries**: The first 4 queries executed successfully, consuming ε = 0.95 total\n",
    "- **Budget Protection**: The 5th query was blocked because it would exceed our ε = 1.0 budget\n",
    "- **Audit Trail**: Every query was logged with its privacy cost and timestamp\n",
    "- **Real-time Monitoring**: You could see the remaining budget decrease after each query\n",
    "\n",
    "**Key Lessons:**\n",
    "1. **Planning is Essential**: You need to plan your query budget allocation in advance\n",
    "2. **Prioritization Matters**: Run the most important queries first in case you run out of budget\n",
    "3. **Automatic Protection**: The system prevents accidental privacy budget overspending\n",
    "4. **Transparency**: Complete audit trails support regulatory compliance and internal governance\n",
    "\n",
    "**Professional Applications:**\n",
    "- **Research Teams**: Allocate budget across different research questions\n",
    "- **Business Analytics**: Balance operational reporting with exploratory analysis  \n",
    "- **Regulatory Compliance**: Demonstrate privacy protection measures to regulators\n",
    "- **Risk Management**: Prevent privacy budget exhaustion that could halt critical operations\n",
    "\n",
    "The visualization shows how your privacy budget depleted over time - this is exactly what privacy engineers monitor in production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cf0732",
   "metadata": {},
   "source": [
    "### Hands-On Budget Management Exercise\n",
    "\n",
    "Now we'll simulate a realistic scenario where a data analyst needs to run multiple queries while staying within their privacy budget. We start with ε = 1.0 total budget and will execute a series of increasingly expensive queries.\n",
    "\n",
    "**The Scenario:**\n",
    "You're a healthcare data analyst who needs to generate a weekly report. You have a limited privacy budget and need to prioritize the most important queries. Watch how the budget decreases with each query and what happens when you try to exceed the limit.\n",
    "\n",
    "**Query Plan:**\n",
    "1. Patient count (ε = 0.2) - Basic statistic\n",
    "2. Average age (ε = 0.3) - Demographics  \n",
    "3. Diabetes count (ε = 0.2) - Health metric\n",
    "4. Age histogram (ε = 0.25) - Distribution analysis\n",
    "5. Treatment costs (ε = 0.3) - Financial analysis\n",
    "\n",
    "Total planned: ε = 1.25 (This exceeds our budget of 1.0 - let's see what happens!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56f34af",
   "metadata": {},
   "source": [
    "## Summary and Reflection\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "In this 30-minute hands-on activity, you have:\n",
    "\n",
    "1. **Implemented Core DP Mechanisms**: You built Laplace and Gaussian noise mechanisms for differential privacy\n",
    "\n",
    "2. **Analyzed Privacy-Utility Trade-offs**: You discovered how different ε values affect query accuracy\n",
    "\n",
    "3. **Compared Noise Mechanisms**: You saw the differences between Laplace and Gaussian mechanisms\n",
    "\n",
    "4. **Managed Privacy Budgets**: You learned how to track and manage privacy budget consumption\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **Privacy Parameter (ε)**: Lower values provide stronger privacy but higher query error\n",
    "- **Sensitivity**: Critical for calculating appropriate noise levels\n",
    "- **Composition**: Multiple queries consume privacy budget additively\n",
    "- **Mechanism Choice**: Laplace vs Gaussian depends on privacy model and composition needs\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "- **Healthcare**: Protecting patient data while enabling medical research\n",
    "- **Finance**: Enabling fraud detection while protecting transaction privacy\n",
    "- **Government**: Publishing census data with individual privacy protection\n",
    "- **Technology**: Improving services while protecting user privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f9f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS CHALLENGE: Design a Private Analytics Dashboard\n",
    "print(\"BONUS CHALLENGE: Design a Private Analytics Dashboard\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nScenario: Hospital Privacy Dashboard\")\n",
    "print(\"Create a dashboard showing key hospital metrics while protecting patient privacy.\")\n",
    "print(\"\\nYour constraints:\")\n",
    "print(\"• Total privacy budget: ε = 2.0\") \n",
    "print(\"• Must provide 5 key statistics for hospital administrators\")\n",
    "print(\"• Balance privacy protection with decision-making utility\")\n",
    "\n",
    "print(\"\\nSuggested statistics to consider:\")\n",
    "print(\"1. Total patient count\")\n",
    "print(\"2. Average patient age\") \n",
    "print(\"3. Percentage with chronic conditions\")\n",
    "print(\"4. Average treatment cost\")\n",
    "print(\"5. Average hospital stay length\")\n",
    "\n",
    "print(\"\\nDesign questions to consider:\")\n",
    "print(\"• Which statistics are most critical for hospital operations?\")\n",
    "print(\"• How should you allocate your ε = 2.0 budget across the 5 queries?\")\n",
    "print(\"• What level of error is acceptable for each metric?\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IMPLEMENTATION SPACE\")\n",
    "print(\"Try creating your own private analytics system below:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Your implementation here - example starter code:\n",
    "def create_hospital_dashboard():\n",
    "    \"\"\"\n",
    "    Create a privacy-preserving hospital dashboard\n",
    "    \n",
    "    Recommended approach:\n",
    "    1. Initialize a new privacy accountant with ε = 2.0 budget\n",
    "    2. Choose 5 key metrics from the patient data  \n",
    "    3. Allocate privacy budget based on importance\n",
    "    4. Execute queries and display results\n",
    "    5. Show remaining budget\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example budget allocation (modify as needed):\n",
    "    # - Patient count: ε = 0.3 (high importance, low sensitivity)\n",
    "    # - Average age: ε = 0.4 (important for planning)  \n",
    "    # - Chronic conditions: ε = 0.5 (critical for resource allocation)\n",
    "    # - Treatment costs: ε = 0.4 (important for financial planning)\n",
    "    # - Stay length: ε = 0.4 (operational planning)\n",
    "    # Total: ε = 2.0\n",
    "    \n",
    "    print(\"Dashboard implementation - your code here!\")\n",
    "    \n",
    "# Call your dashboard function\n",
    "create_hospital_dashboard()\n",
    "\n",
    "print(\"\\nActivity Complete!\")\n",
    "print(\"You're now ready to implement differential privacy in real-world scenarios.\")\n",
    "print(\"Key skills gained:\")\n",
    "print(\"• Understanding privacy-utility trade-offs\")\n",
    "print(\"• Implementing DP mechanisms\")  \n",
    "print(\"• Managing privacy budgets\")\n",
    "print(\"• Comparing noise mechanisms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116714bf",
   "metadata": {},
   "source": [
    "## Chapter 7 Knowledge Assessment\n",
    "\n",
    "Test your understanding of differential privacy, ethical AI frameworks, and privacy-preserving techniques with our interactive quiz. The quiz covers key concepts from both the chapter content and hands-on activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7ffed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch Chapter 7 Quiz\n",
    "import webbrowser\n",
    "import os\n",
    "\n",
    "quiz_file = 'chapter7_quiz.html'\n",
    "file_path = os.path.abspath(quiz_file)\n",
    "webbrowser.open(f'file://{file_path}')\n",
    "\n",
    "print(\"Opening Chapter 7 Quiz in your web browser...\")\n",
    "print(\"The quiz covers differential privacy, ethical AI, and privacy-preserving techniques.\")\n",
    "print(\"Complete the quiz to test your understanding of the chapter content!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
