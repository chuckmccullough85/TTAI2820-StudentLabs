{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93ccee48",
   "metadata": {},
   "source": [
    "# Chapter 5 Activity: AI Security Forensics Investigation\n",
    "\n",
    "## CSI Cyber - Investigating an AI Model Compromise\n",
    "\n",
    "**Estimated Time:** 30 minutes  \n",
    "**Learning Objectives:**\n",
    "- Perform basic AI model forensics analysis\n",
    "- Detect potential adversarial attacks on ML models\n",
    "- Analyze suspicious model behavior patterns\n",
    "- Generate investigation reports with evidence\n",
    "\n",
    "---\n",
    "\n",
    "## Scenario Background\n",
    "\n",
    "You are a cybersecurity analyst at SecureAI Corp. The company's production image classification model has been exhibiting unusual behavior - it's misclassifying certain images in ways that seem deliberate rather than random. Management suspects a potential adversarial attack or model poisoning incident.\n",
    "\n",
    "Your mission: Investigate the compromised AI model to determine:\n",
    "1. What type of attack occurred\n",
    "2. The scope of the compromise\n",
    "3. Evidence collection for incident response\n",
    "\n",
    "Let's begin our investigation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408626a0",
   "metadata": {},
   "source": [
    "## Step 1: Set Up the Investigation Environment\n",
    "\n",
    "First, let's import the necessary libraries and set up our forensics toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53061867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üîç AI Forensics Investigation Toolkit Loaded\")\n",
    "print(f\"Investigation started at: {datetime.now()}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4621cdd",
   "metadata": {},
   "source": [
    "## Step 2: Load the \"Compromised\" Model and Data\n",
    "\n",
    "We'll simulate a scenario where we have access to a model that's been potentially compromised, along with clean and suspicious datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ba13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the original \"clean\" dataset with enhanced separability\n",
    "np.random.seed(42)\n",
    "X_clean, y_clean = make_classification(\n",
    "    n_samples=1200,  # Increased sample size\n",
    "    n_features=20, \n",
    "    n_informative=18,  # More informative features (was 15)\n",
    "    n_redundant=2,     # Fewer redundant features (was 5) \n",
    "    n_classes=3, \n",
    "    n_clusters_per_class=1,  # Better class separation\n",
    "    class_sep=1.5,    # Enhanced class separation\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split into train/test\n",
    "X_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(\n",
    "    X_clean, y_clean, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train a \"clean\" baseline model\n",
    "clean_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clean_model.fit(X_train_clean, y_train_clean)\n",
    "\n",
    "clean_baseline_accuracy = accuracy_score(y_test_clean, clean_model.predict(X_test_clean))\n",
    "\n",
    "print(\"üìä Enhanced clean baseline model trained\")\n",
    "print(f\"Clean model accuracy: {clean_baseline_accuracy:.3f}\")\n",
    "print(f\"Training samples: {len(X_train_clean)}\")\n",
    "print(f\"Test samples: {len(X_test_clean)}\")\n",
    "print(f\"Class distribution: {np.bincount(y_train_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e614ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create a \"compromised\" dataset with poisoned samples\n",
    "# This simulates a data poisoning attack\n",
    "\n",
    "def create_poisoned_data(X, y, poison_rate=0.25):\n",
    "    \"\"\"Create poisoned training data with aggressive label flipping and feature manipulation\"\"\"\n",
    "    X_poisoned = X.copy()\n",
    "    y_poisoned = y.copy()\n",
    "    \n",
    "    # Select samples to poison (increased rate for more impact)\n",
    "    n_poison = int(len(X) * poison_rate)\n",
    "    poison_indices = np.random.choice(len(X), n_poison, replace=False)\n",
    "    \n",
    "    # More aggressive poisoning strategy\n",
    "    for idx in poison_indices:\n",
    "        # Targeted attack: specifically target class 0 ‚Üí class 2 (most confusing)\n",
    "        original_label = y_poisoned[idx]\n",
    "        if original_label == 0:\n",
    "            y_poisoned[idx] = 2  # High-confidence wrong classification\n",
    "        elif original_label == 1:\n",
    "            y_poisoned[idx] = 0  # Create systematic confusion\n",
    "        else:  # original_label == 2\n",
    "            y_poisoned[idx] = 1  # Circular confusion pattern\n",
    "        \n",
    "        # Add more significant feature perturbations to create stronger trigger patterns\n",
    "        # This simulates a backdoor attack with detectable patterns\n",
    "        for feature_idx in range(X_poisoned.shape[1]):\n",
    "            if feature_idx < 5:  # Modify first 5 features more aggressively\n",
    "                X_poisoned[idx, feature_idx] += np.random.normal(0, 0.8)  # Larger noise\n",
    "            else:\n",
    "                X_poisoned[idx, feature_idx] += np.random.normal(0, 0.3)  # Moderate noise\n",
    "        \n",
    "        # Add systematic bias to create detectable anomalies\n",
    "        if idx % 3 == 0:  # Every third poisoned sample gets additional manipulation\n",
    "            X_poisoned[idx] += np.random.uniform(-1.0, 1.0, X_poisoned[idx].shape)\n",
    "    \n",
    "    return X_poisoned, y_poisoned, poison_indices\n",
    "\n",
    "# Create poisoned training data with higher impact\n",
    "X_train_poisoned, y_train_poisoned, poison_indices = create_poisoned_data(\n",
    "    X_train_clean, y_train_clean, poison_rate=0.25  # Increased from 0.15\n",
    ")\n",
    "\n",
    "# Train the \"compromised\" model\n",
    "compromised_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "compromised_model.fit(X_train_poisoned, y_train_poisoned)\n",
    "\n",
    "print(\"üö® HIGH-IMPACT Compromised model created and trained\")\n",
    "print(f\"Number of poisoned samples: {len(poison_indices)}\")\n",
    "print(f\"Poison rate: {len(poison_indices)/len(X_train_clean)*100:.1f}%\")\n",
    "print(\"‚ö†Ô∏è  Enhanced poisoning strategy: Targeted label flipping + aggressive feature manipulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f9fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative poisoning strategy: Subtle but systematic attack\n",
    "def create_subtle_poisoned_data(X, y, poison_rate=0.20):\n",
    "    \"\"\"Create poisoned data with subtle but systematic pattern\"\"\"\n",
    "    X_poisoned = X.copy()\n",
    "    y_poisoned = y.copy()\n",
    "    \n",
    "    n_poison = int(len(X) * poison_rate)\n",
    "    poison_indices = np.random.choice(len(X), n_poison, replace=False)\n",
    "    \n",
    "    # Create a systematic trigger pattern\n",
    "    trigger_features = [0, 5, 10]  # Specific features to modify\n",
    "    \n",
    "    for idx in poison_indices:\n",
    "        # Systematic label flip: all poisoned samples ‚Üí class 1\n",
    "        y_poisoned[idx] = 1\n",
    "        \n",
    "        # Add trigger pattern to specific features\n",
    "        for feat_idx in trigger_features:\n",
    "            X_poisoned[idx, feat_idx] += 2.0  # Consistent trigger value\n",
    "        \n",
    "        # Add small random noise to other features to mask the attack\n",
    "        for feat_idx in range(X_poisoned.shape[1]):\n",
    "            if feat_idx not in trigger_features:\n",
    "                X_poisoned[idx, feat_idx] += np.random.normal(0, 0.1)\n",
    "    \n",
    "    return X_poisoned, y_poisoned, poison_indices\n",
    "\n",
    "# Create both aggressive and subtle poisoned versions\n",
    "print(\"üéØ Creating Multiple Attack Scenarios...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Scenario 1: Aggressive attack (already created above)\n",
    "print(\"‚úÖ Scenario 1: Aggressive poisoning (25% rate, high noise)\")\n",
    "\n",
    "# Scenario 2: Subtle systematic attack  \n",
    "X_train_subtle, y_train_subtle, subtle_poison_indices = create_subtle_poisoned_data(\n",
    "    X_train_clean, y_train_clean, poison_rate=0.20\n",
    ")\n",
    "\n",
    "subtle_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "subtle_model.fit(X_train_subtle, y_train_subtle)\n",
    "\n",
    "print(f\"‚úÖ Scenario 2: Subtle systematic attack (20% rate, trigger patterns)\")\n",
    "print(f\"   Poisoned samples: {len(subtle_poison_indices)}\")\n",
    "\n",
    "# Quick comparison of attack impacts\n",
    "clean_acc = accuracy_score(y_test_clean, clean_model.predict(X_test_clean))\n",
    "aggressive_acc = accuracy_score(y_test_clean, compromised_model.predict(X_test_clean))  \n",
    "subtle_acc = accuracy_score(y_test_clean, subtle_model.predict(X_test_clean))\n",
    "\n",
    "print(f\"\\nüìä Initial Impact Assessment:\")\n",
    "print(f\"   Clean model:      {clean_acc:.3f}\")\n",
    "print(f\"   Aggressive poison: {aggressive_acc:.3f} (impact: {(clean_acc-aggressive_acc)*100:.1f}%)\")\n",
    "print(f\"   Subtle poison:     {subtle_acc:.3f} (impact: {(clean_acc-subtle_acc)*100:.1f}%)\")\n",
    "print(f\"\\nüîç Proceeding with AGGRESSIVE attack scenario for main investigation...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebce2e74",
   "metadata": {},
   "source": [
    "## Step 3: Forensic Analysis - Model Performance Comparison\n",
    "\n",
    "Let's begin our investigation by comparing the performance of the suspected compromised model against our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c339fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performances with enhanced forensic analysis\n",
    "clean_pred = clean_model.predict(X_test_clean)\n",
    "compromised_pred = compromised_model.predict(X_test_clean)\n",
    "\n",
    "clean_accuracy = accuracy_score(y_test_clean, clean_pred)\n",
    "compromised_accuracy = accuracy_score(y_test_clean, compromised_pred)\n",
    "\n",
    "print(\"üîç ENHANCED FORENSIC ANALYSIS - Model Performance\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"Clean Model Accuracy:        {clean_accuracy:.3f}\")\n",
    "print(f\"Compromised Model Accuracy:  {compromised_accuracy:.3f}\")\n",
    "print(f\"Performance Degradation:     {(clean_accuracy - compromised_accuracy):.3f}\")\n",
    "print(f\"Degradation Percentage:      {((clean_accuracy - compromised_accuracy)/clean_accuracy)*100:.1f}%\")\n",
    "\n",
    "# Enhanced prediction analysis\n",
    "different_predictions = np.sum(clean_pred != compromised_pred)\n",
    "print(f\"\\nPrediction Discrepancies:    {different_predictions}/{len(y_test_clean)}\")\n",
    "print(f\"Discrepancy Rate:           {(different_predictions/len(y_test_clean))*100:.1f}%\")\n",
    "\n",
    "# Class-specific analysis\n",
    "print(f\"\\nüìä Class-Specific Impact Analysis:\")\n",
    "for class_label in [0, 1, 2]:\n",
    "    class_mask = y_test_clean == class_label\n",
    "    if np.sum(class_mask) > 0:\n",
    "        class_clean_acc = accuracy_score(y_test_clean[class_mask], clean_pred[class_mask])\n",
    "        class_comp_acc = accuracy_score(y_test_clean[class_mask], compromised_pred[class_mask])\n",
    "        class_impact = (class_clean_acc - class_comp_acc) * 100\n",
    "        print(f\"   Class {class_label}: Clean={class_clean_acc:.3f}, Compromised={class_comp_acc:.3f}, Impact={class_impact:+.1f}%\")\n",
    "\n",
    "# Advanced threat assessment\n",
    "degradation_threshold = 0.08  # Lowered threshold for enhanced detection\n",
    "if compromised_accuracy < clean_accuracy - degradation_threshold:\n",
    "    threat_level = \"üö® CRITICAL\"\n",
    "    print(f\"\\n{threat_level}: Severe performance degradation detected!\")\n",
    "    print(\"   Evidence suggests AGGRESSIVE data poisoning attack\")\n",
    "elif compromised_accuracy < clean_accuracy - 0.05:\n",
    "    threat_level = \"‚ö†Ô∏è  HIGH\"\n",
    "    print(f\"\\n{threat_level}: Significant performance degradation detected!\")\n",
    "    print(\"   Evidence suggests moderate data poisoning attack\")\n",
    "else:\n",
    "    threat_level = \"‚úÖ LOW\"\n",
    "    print(f\"\\n{threat_level}: Performance degradation within acceptable range.\")\n",
    "\n",
    "# Confusion matrix comparison\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "clean_cm = confusion_matrix(y_test_clean, clean_pred)\n",
    "comp_cm = confusion_matrix(y_test_clean, compromised_pred)\n",
    "\n",
    "print(f\"\\nüéØ Confusion Matrix Analysis:\")\n",
    "print(f\"Clean Model Confusion Matrix:\")\n",
    "print(clean_cm)\n",
    "print(f\"\\nCompromised Model Confusion Matrix:\")\n",
    "print(comp_cm)\n",
    "print(f\"\\nDifference Matrix (Clean - Compromised):\")\n",
    "print(clean_cm - comp_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92cfd16",
   "metadata": {},
   "source": [
    "## Step 4: Deep Dive Analysis - Feature Importance Investigation\n",
    "\n",
    "Let's analyze if the attack has affected the model's feature importance patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10712309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature importances between clean and compromised models\n",
    "clean_importance = clean_model.feature_importances_\n",
    "compromised_importance = compromised_model.feature_importances_\n",
    "\n",
    "# Calculate importance differences\n",
    "importance_diff = np.abs(clean_importance - compromised_importance)\n",
    "significant_changes = importance_diff > 0.02  # 2% threshold\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Feature importance comparison\n",
    "features = range(len(clean_importance))\n",
    "ax1.bar([f-0.2 for f in features], clean_importance, width=0.4, label='Clean Model', alpha=0.7)\n",
    "ax1.bar([f+0.2 for f in features], compromised_importance, width=0.4, label='Compromised Model', alpha=0.7)\n",
    "ax1.set_xlabel('Feature Index')\n",
    "ax1.set_ylabel('Importance')\n",
    "ax1.set_title('Feature Importance Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Highlight significantly changed features\n",
    "for i, changed in enumerate(significant_changes):\n",
    "    if changed:\n",
    "        ax1.axvline(x=i, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Difference plot\n",
    "ax2.bar(features, importance_diff, color='orange', alpha=0.7)\n",
    "ax2.axhline(y=0.02, color='red', linestyle='--', label='Significance Threshold')\n",
    "ax2.set_xlabel('Feature Index')\n",
    "ax2.set_ylabel('Importance Difference')\n",
    "ax2.set_title('Feature Importance Changes')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Features with significant changes: {np.sum(significant_changes)}\")\n",
    "if np.sum(significant_changes) > 0:\n",
    "    print(f\"Changed feature indices: {np.where(significant_changes)[0].tolist()}\")\n",
    "    print(f\"Maximum importance change: {np.max(importance_diff):.4f}\")\n",
    "    print(\"\\n‚ö†Ô∏è  EVIDENCE: Feature importance patterns have been altered!\")\n",
    "else:\n",
    "    print(\"‚úÖ No significant feature importance changes detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5d4f0c",
   "metadata": {},
   "source": [
    "## Step 5: Advanced Forensics - Anomaly Detection in Training Data\n",
    "\n",
    "Now let's investigate the training data itself to look for evidence of data poisoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b155af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze training data for anomalies\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_poisoned)\n",
    "\n",
    "# Apply anomaly detection\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "anomaly_labels = iso_forest.fit_predict(X_train_scaled)\n",
    "\n",
    "# Identify anomalous samples\n",
    "anomalous_indices = np.where(anomaly_labels == -1)[0]\n",
    "normal_indices = np.where(anomaly_labels == 1)[0]\n",
    "\n",
    "# Check overlap with known poisoned samples\n",
    "detected_poison = np.intersect1d(anomalous_indices, poison_indices)\n",
    "missed_poison = np.setdiff1d(poison_indices, anomalous_indices)\n",
    "false_positives = np.setdiff1d(anomalous_indices, poison_indices)\n",
    "\n",
    "print(\"üîç ANOMALY DETECTION ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Total training samples:           {len(X_train_poisoned)}\")\n",
    "print(f\"Detected anomalous samples:       {len(anomalous_indices)}\")\n",
    "print(f\"Known poisoned samples:           {len(poison_indices)}\")\n",
    "print(f\"Correctly detected poison:        {len(detected_poison)}\")\n",
    "print(f\"Missed poisoned samples:          {len(missed_poison)}\")\n",
    "print(f\"False positive detections:        {len(false_positives)}\")\n",
    "\n",
    "# Calculate detection metrics\n",
    "if len(poison_indices) > 0:\n",
    "    detection_rate = len(detected_poison) / len(poison_indices)\n",
    "    print(f\"\\nPoison Detection Rate:            {detection_rate:.3f} ({detection_rate*100:.1f}%)\")\n",
    "    \n",
    "    if detection_rate > 0.5:\n",
    "        print(\"\\nüö® EVIDENCE: High anomaly detection rate suggests data poisoning!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Low detection rate - poisoning may be sophisticated or minimal.\")\n",
    "\n",
    "# Visualize anomalous vs normal samples\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Feature distribution comparison\n",
    "feature_idx = 0  # Analyze first feature\n",
    "axes[0].hist(X_train_poisoned[normal_indices, feature_idx], bins=20, alpha=0.7, label='Normal', density=True)\n",
    "axes[0].hist(X_train_poisoned[anomalous_indices, feature_idx], bins=20, alpha=0.7, label='Anomalous', density=True)\n",
    "axes[0].set_title(f'Feature {feature_idx} Distribution')\n",
    "axes[0].set_xlabel('Feature Value')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Sample indices plot\n",
    "sample_indices = range(len(X_train_poisoned))\n",
    "colors = ['blue' if i in normal_indices else 'red' for i in sample_indices]\n",
    "axes[1].scatter(sample_indices, X_train_poisoned[:, 0], c=colors, alpha=0.6, s=10)\n",
    "axes[1].set_title('Sample Anomaly Status')\n",
    "axes[1].set_xlabel('Sample Index')\n",
    "axes[1].set_ylabel('Feature 0 Value')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Detection overlap visualization\n",
    "categories = ['Detected\\nPoison', 'Missed\\nPoison', 'False\\nPositives', 'True\\nNegatives']\n",
    "counts = [len(detected_poison), len(missed_poison), len(false_positives), \n",
    "          len(X_train_poisoned) - len(poison_indices) - len(false_positives)]\n",
    "colors_bar = ['green', 'red', 'orange', 'blue']\n",
    "axes[2].bar(categories, counts, color=colors_bar, alpha=0.7)\n",
    "axes[2].set_title('Detection Performance')\n",
    "axes[2].set_ylabel('Number of Samples')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6b6c1d",
   "metadata": {},
   "source": [
    "## Step 6: Generate Investigation Report\n",
    "\n",
    "Let's compile our findings into a comprehensive forensics report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb9b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive investigation report\n",
    "def generate_investigation_report():\n",
    "    report = []\n",
    "    report.append(\"=\"*80)\n",
    "    report.append(\"üîç AI SECURITY FORENSICS INVESTIGATION REPORT\")\n",
    "    report.append(\"=\"*80)\n",
    "    report.append(f\"Investigation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    report.append(f\"Analyst: AI Security Specialist\")\n",
    "    report.append(f\"Case ID: CSI-AI-2025-001\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Executive Summary\n",
    "    report.append(\"üìã EXECUTIVE SUMMARY\")\n",
    "    report.append(\"-\" * 30)\n",
    "    \n",
    "    if compromised_accuracy < clean_accuracy - 0.05:\n",
    "        threat_level = \"HIGH\"\n",
    "        status = \"CONFIRMED COMPROMISE\"\n",
    "    elif len(detected_poison) > len(poison_indices) * 0.5:\n",
    "        threat_level = \"MEDIUM\"\n",
    "        status = \"SUSPECTED COMPROMISE\"\n",
    "    else:\n",
    "        threat_level = \"LOW\"\n",
    "        status = \"MINIMAL EVIDENCE\"\n",
    "    \n",
    "    report.append(f\"Threat Level: {threat_level}\")\n",
    "    report.append(f\"Investigation Status: {status}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Technical Findings\n",
    "    report.append(\"üî¨ TECHNICAL FINDINGS\")\n",
    "    report.append(\"-\" * 30)\n",
    "    report.append(f\"Model Performance Analysis:\")\n",
    "    report.append(f\"  ‚Ä¢ Clean model accuracy: {clean_accuracy:.3f}\")\n",
    "    report.append(f\"  ‚Ä¢ Compromised model accuracy: {compromised_accuracy:.3f}\")\n",
    "    report.append(f\"  ‚Ä¢ Performance degradation: {((clean_accuracy - compromised_accuracy)/clean_accuracy)*100:.1f}%\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    report.append(f\"Data Poisoning Analysis:\")\n",
    "    report.append(f\"  ‚Ä¢ Total training samples: {len(X_train_poisoned)}\")\n",
    "    report.append(f\"  ‚Ä¢ Detected anomalous samples: {len(anomalous_indices)}\")\n",
    "    report.append(f\"  ‚Ä¢ Known poisoned samples: {len(poison_indices)}\")\n",
    "    if len(poison_indices) > 0:\n",
    "        report.append(f\"  ‚Ä¢ Detection rate: {(len(detected_poison)/len(poison_indices))*100:.1f}%\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    report.append(f\"Feature Analysis:\")\n",
    "    report.append(f\"  ‚Ä¢ Features with significant changes: {np.sum(significant_changes)}\")\n",
    "    report.append(f\"  ‚Ä¢ Maximum importance change: {np.max(importance_diff):.4f}\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Evidence Summary\n",
    "    report.append(\"üìä EVIDENCE SUMMARY\")\n",
    "    report.append(\"-\" * 30)\n",
    "    evidence_count = 0\n",
    "    \n",
    "    if compromised_accuracy < clean_accuracy - 0.05:\n",
    "        report.append(\"‚úÖ Evidence #1: Significant model performance degradation\")\n",
    "        evidence_count += 1\n",
    "    \n",
    "    if np.sum(significant_changes) > 0:\n",
    "        report.append(\"‚úÖ Evidence #2: Altered feature importance patterns\")\n",
    "        evidence_count += 1\n",
    "    \n",
    "    if len(detected_poison) > len(poison_indices) * 0.3:\n",
    "        report.append(\"‚úÖ Evidence #3: Anomalous training data detected\")\n",
    "        evidence_count += 1\n",
    "    \n",
    "    if evidence_count == 0:\n",
    "        report.append(\"‚ö†Ô∏è  No strong evidence of compromise detected\")\n",
    "    \n",
    "    report.append(f\"\\nTotal evidence items: {evidence_count}/3\")\n",
    "    report.append(\"\")\n",
    "    \n",
    "    # Recommendations\n",
    "    report.append(\"üõ°Ô∏è RECOMMENDATIONS\")\n",
    "    report.append(\"-\" * 30)\n",
    "    \n",
    "    if threat_level == \"HIGH\":\n",
    "        report.append(\"‚Ä¢ IMMEDIATE: Quarantine the compromised model\")\n",
    "        report.append(\"‚Ä¢ IMMEDIATE: Investigate data sources for integrity\")\n",
    "        report.append(\"‚Ä¢ HIGH: Retrain model with verified clean data\")\n",
    "        report.append(\"‚Ä¢ HIGH: Implement enhanced monitoring systems\")\n",
    "    elif threat_level == \"MEDIUM\":\n",
    "        report.append(\"‚Ä¢ HIGH: Conduct deeper forensic analysis\")\n",
    "        report.append(\"‚Ä¢ MEDIUM: Review training data provenance\")\n",
    "        report.append(\"‚Ä¢ MEDIUM: Enhance model validation procedures\")\n",
    "    else:\n",
    "        report.append(\"‚Ä¢ LOW: Continue routine monitoring\")\n",
    "        report.append(\"‚Ä¢ LOW: Document findings for future reference\")\n",
    "    \n",
    "    report.append(\"\")\n",
    "    report.append(\"=\" * 80)\n",
    "    report.append(\"End of Report\")\n",
    "    \n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "# Generate and display the report\n",
    "investigation_report = generate_investigation_report()\n",
    "print(investigation_report)\n",
    "\n",
    "# Save report to file (simulated)\n",
    "print(\"\\nüíæ Report saved to: investigation_report_CSI-AI-2025-001.txt\")\n",
    "print(\"üìß Report ready for incident response team review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9975a47",
   "metadata": {},
   "source": [
    "## Step 7: Activity Questions and Analysis\n",
    "\n",
    "Answer the following questions based on your investigation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc69f7",
   "metadata": {},
   "source": [
    "### Investigation Questions\n",
    "\n",
    "**Question 1:** What type of attack do you believe occurred based on the evidence?\n",
    "\n",
    "*Your Answer:* [Write your analysis here]\n",
    "\n",
    "**Question 2:** How effective was the anomaly detection in identifying poisoned samples?\n",
    "\n",
    "*Your Answer:* [Write your analysis here]\n",
    "\n",
    "**Question 3:** What additional forensic techniques could you employ to gather more evidence?\n",
    "\n",
    "*Your Answer:* [Write your analysis here]\n",
    "\n",
    "**Question 4:** Based on your threat level assessment, what immediate actions should the organization take?\n",
    "\n",
    "*Your Answer:* [Write your analysis here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ca4fc1",
   "metadata": {},
   "source": [
    "## Activity Summary\n",
    "\n",
    "Congratulations! You've completed your first AI security forensics investigation. In this activity, you:\n",
    "\n",
    "‚úÖ **Performed model performance analysis** to detect potential compromise  \n",
    "‚úÖ **Analyzed feature importance changes** that may indicate attack vectors  \n",
    "‚úÖ **Applied anomaly detection** to identify suspicious training data  \n",
    "‚úÖ **Generated a comprehensive investigation report** with evidence and recommendations  \n",
    "‚úÖ **Assessed threat levels** and provided actionable security guidance  \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Multiple Evidence Sources**: Effective AI forensics requires analyzing multiple aspects (performance, features, data)\n",
    "2. **Quantitative Analysis**: Use metrics and statistical analysis to support conclusions\n",
    "3. **Documentation**: Proper reporting is crucial for incident response and legal proceedings\n",
    "4. **Proactive Detection**: Anomaly detection can help identify attacks before they cause significant damage\n",
    "\n",
    "### Next Steps:\n",
    "- Practice with different attack scenarios (adversarial examples, model extraction)\n",
    "- Learn advanced forensics techniques (model archaeology, gradient analysis)\n",
    "- Explore automated threat hunting for AI systems\n",
    "\n",
    "---\n",
    "\n",
    "**Investigation Complete** üéØ  \n",
    "*Time to move on to more advanced CSI Cyber techniques!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3dbc73",
   "metadata": {},
   "source": [
    "## üß† Chapter 5 Self-Assessment Quiz\n",
    "\n",
    "Test your understanding of AI security forensics and investigation with our interactive quiz! This comprehensive assessment covers:\n",
    "\n",
    "### üìã **Quiz Coverage**\n",
    "- **AI Forensics Investigation** - Data poisoning detection and anomaly analysis\n",
    "- **Digital Evidence Collection** - Model parameters, training data, and system artifacts\n",
    "- **Emerging AI Threats** - LLM exploitation, federated learning attacks, autonomous campaigns\n",
    "- **Incident Response** - AI-specific containment, recovery, and threat hunting\n",
    "- **Supply Chain Security** - Pre-trained model risks and backdoor detection\n",
    "\n",
    "### üéØ **What You'll Test**\n",
    "- Understanding of AI forensics frameworks and methodologies\n",
    "- Knowledge of data poisoning detection techniques\n",
    "- Practical investigation skills from the hands-on activities\n",
    "- Emerging threat landscape and attack vector awareness\n",
    "\n",
    "**üìä Quiz Format:** 10 multiple-choice questions with detailed explanations  \n",
    "**‚è±Ô∏è Estimated Time:** 15-20 minutes  \n",
    "**üéì Passing Score:** 70% (7/10 questions correct)\n",
    "\n",
    "Ready to test your AI security investigation knowledge? Run the cell below to launch the interactive quiz!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b245a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "import os\n",
    "\n",
    "quiz_file = 'chapter5_quiz.html'\n",
    "file_path = os.path.abspath(quiz_file)\n",
    "webbrowser.open(f'file://{file_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
