{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "264a47dc",
   "metadata": {},
   "source": [
    "# Chapter 1: Introduction to AI Security - Hands-On Lab\n",
    "\n",
    "Welcome to your first hands-on experience with AI security! This interactive lab complements the Chapter 1 lecture by giving you practical experience with real AI security concepts.\n",
    "\n",
    "## üéØ Lab Objectives (30-45 minutes)\n",
    "By completing this lab, you will:\n",
    "- **Experience AI as both a security tool and security target** through hands-on exercises\n",
    "- **Implement basic AI security controls** including input validation and monitoring\n",
    "- **Identify security vulnerabilities** in a real machine learning model\n",
    "- **Apply the AI Security Framework** covered in the lecture slides\n",
    "- **Understand why traditional security isn't enough** for AI systems\n",
    "\n",
    "## üöÄ What You'll Build\n",
    "1. **A fraud detection AI system** that demonstrates real-world security challenges\n",
    "2. **Security wrapper** with input validation, rate limiting, and monitoring\n",
    "3. **Security assessment dashboard** to track threats and model behavior\n",
    "4. **Mini penetration test** against your own AI model\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of machine learning concepts\n",
    "- Familiarity with Python programming\n",
    "- Completion of Chapter 1 lecture slides\n",
    "\n",
    "## ‚ö†Ô∏è Lab Safety Note\n",
    "This lab uses synthetic data and simulated attacks in a safe environment. The techniques demonstrated here should only be used for educational purposes and authorized security testing.\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d8aad",
   "metadata": {},
   "source": [
    "## üîß Lab Setup (2 minutes)\n",
    "\n",
    "First, let's set up our environment with the required packages. All dependencies should already be installed from your requirements.txt file.\n",
    "\n",
    "**Run the cell below to import libraries and verify your setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f26aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.datasets import make_classification, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06381b4",
   "metadata": {},
   "source": [
    "## üéØ Activity 1: Building Our Target - AI Fraud Detection System (8 minutes)\n",
    "\n",
    "Let's create a machine learning system that we'll use to demonstrate AI security concepts. This mirrors real-world scenarios where **AI systems become security targets**.\n",
    "\n",
    "**Key Chapter 1 Concept**: AI systems face unique threats that traditional security can't address. We'll build a fraud detection model that represents a typical enterprise AI system.\n",
    "\n",
    "**Your Task**: Create a synthetic fraud detection dataset and understand why this system needs specialized security measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f200a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic fraud detection dataset\n",
    "def create_fraud_dataset(n_samples=10000, n_features=20, n_informative=10):\n",
    "    \"\"\"\n",
    "    Create a synthetic fraud detection dataset\n",
    "    \"\"\"\n",
    "    X, y = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_informative=n_informative,\n",
    "        n_redundant=5,\n",
    "        n_clusters_per_class=2,\n",
    "        class_sep=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Create feature names\n",
    "    feature_names = [\n",
    "        'transaction_amount', 'account_age', 'num_previous_transactions',\n",
    "        'time_since_last_transaction', 'merchant_risk_score', 'location_risk_score',\n",
    "        'device_risk_score', 'behavioral_score', 'network_risk_score',\n",
    "        'payment_method_risk', 'transaction_velocity', 'amount_deviation',\n",
    "        'time_of_day_risk', 'day_of_week_risk', 'seasonal_risk',\n",
    "        'customer_tier', 'account_balance', 'credit_score',\n",
    "        'geographic_risk', 'channel_risk'\n",
    "    ]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    df['is_fraud'] = y\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate the dataset\n",
    "fraud_data = create_fraud_dataset()\n",
    "print(f\"Dataset created with {len(fraud_data)} samples\")\n",
    "print(f\"Fraud rate: {fraud_data['is_fraud'].mean():.2%}\")\n",
    "print(\"\\nDataset preview:\")\n",
    "print(fraud_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d35495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset to understand our AI system\n",
    "print(\"üìä Analyzing our fraud detection dataset...\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"Dataset size: {len(fraud_data):,} transactions\")\n",
    "print(f\"Fraud rate: {fraud_data['is_fraud'].mean():.1%}\")\n",
    "print(f\"Features: {fraud_data.shape[1]-1} risk indicators\")\n",
    "\n",
    "# Class distribution\n",
    "class_counts = fraud_data['is_fraud'].value_counts()\n",
    "print(f\"\\nClass Distribution:\")\n",
    "print(f\"  Legitimate transactions: {class_counts[0]:,}\")\n",
    "print(f\"  Fraudulent transactions: {class_counts[1]:,}\")\n",
    "\n",
    "# Create a simple visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Class distribution pie chart\n",
    "axes[0].pie(class_counts.values, labels=['Legitimate', 'Fraud'], autopct='%1.1f%%', \n",
    "           colors=['lightblue', 'lightcoral'])\n",
    "axes[0].set_title('Transaction Class Distribution')\n",
    "\n",
    "# Feature correlation heatmap (first 8 features for readability)\n",
    "corr_matrix = fraud_data.iloc[:, :8].corr()\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='RdBu_r', center=0, ax=axes[1])\n",
    "axes[1].set_title('Feature Correlations (Sample)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Key Observation: This balanced dataset means our AI system will face sophisticated attacks!\")\n",
    "print(\"Real fraud datasets are highly imbalanced (< 1% fraud), making them even more vulnerable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e54852",
   "metadata": {},
   "source": [
    "## üéØ Activity 2: Training Our AI System (5 minutes)\n",
    "\n",
    "Now let's train our fraud detection model. We'll use this as our **target system** for security testing.\n",
    "\n",
    "**Key Chapter 1 Concept**: Every AI model creates an expanded attack surface. Unlike traditional software, attackers can manipulate AI through data, inputs, and even model queries.\n",
    "\n",
    "**Your Task**: Train a baseline model and identify why it needs security protection beyond traditional measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a85e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "X = fraud_data.drop('is_fraud', axis=1)\n",
    "y = fraud_data['is_fraud']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f6d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our fraud detection model\n",
    "print(\"ü§ñ Training fraud detection AI system...\")\n",
    "\n",
    "# Train a Random Forest model (commonly used in production)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"‚úÖ Model trained successfully!\")\n",
    "print(f\"üìà Accuracy: {accuracy:.1%}\")\n",
    "print(f\"üéØ This model is now ready for deployment... but is it secure?\")\n",
    "\n",
    "# Show what makes this system vulnerable\n",
    "print(f\"\\n‚ö†Ô∏è  AI Security Concerns:\")\n",
    "print(f\"   ‚Ä¢ {X.shape[1]} input features = {X.shape[1]} potential attack vectors\")\n",
    "print(f\"   ‚Ä¢ Model processes {len(X_test):,} predictions in this test alone\")\n",
    "print(f\"   ‚Ä¢ No input validation or monitoring currently implemented\")\n",
    "print(f\"   ‚Ä¢ Traditional firewalls can't detect adversarial inputs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62430526",
   "metadata": {},
   "source": [
    "    Note - if the output from the cell above ^ is partially duplicated, it is a known bug in the VSCode Jupyter extension.  It is fixed in the preview version "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55f8351",
   "metadata": {},
   "source": [
    "## üõ°Ô∏è Activity 3: Implementing AI Security Controls (10 minutes)\n",
    "\n",
    "Now comes the critical part - securing our AI system! This demonstrates the **AI Security Framework** from the lecture.\n",
    "\n",
    "**Key Chapter 1 Concepts**:\n",
    "- **Why Traditional Security Falls Short**: Firewalls can't detect adversarial inputs\n",
    "- **AI-Specific Threats**: Model extraction, adversarial examples, data poisoning\n",
    "- **Defense in Depth**: Multiple layers of AI security controls\n",
    "\n",
    "**Your Task**: Build a security wrapper that implements the controls discussed in the lecture slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df611b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecureAIModel:\n",
    "    \"\"\"\n",
    "    A security wrapper implementing AI Security Framework controls from Chapter 1\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, scaler, feature_names):\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        # Security controls from Chapter 1 framework\n",
    "        self.prediction_log = []\n",
    "        self.access_log = []\n",
    "        self.rate_limit_counter = {}\n",
    "        self.max_requests_per_minute = 100\n",
    "        self.suspicious_activity_detected = []\n",
    "        \n",
    "        # Define normal input ranges (baseline security)\n",
    "        self.input_bounds = {\n",
    "            'min_values': X.min().values,\n",
    "            'max_values': X.max().values,\n",
    "            'expected_shape': X.shape[1]\n",
    "        }\n",
    "        print(\"üõ°Ô∏è Security wrapper initialized with AI-specific controls!\")\n",
    "    \n",
    "    def validate_input(self, X_input, user_id=None):\n",
    "        \"\"\"\n",
    "        AI-specific input validation (addresses Chapter 1 threat: Adversarial Inputs)\n",
    "        \"\"\"\n",
    "        import time\n",
    "        \n",
    "        # Log access for monitoring\n",
    "        access_time = time.time()\n",
    "        self.access_log.append({\n",
    "            'user_id': user_id,\n",
    "            'timestamp': access_time,\n",
    "            'input_shape': X_input.shape if hasattr(X_input, 'shape') else None\n",
    "        })\n",
    "        \n",
    "        # Rate limiting (prevents model extraction attacks)\n",
    "        if user_id:\n",
    "            current_minute = int(access_time // 60)\n",
    "            if user_id not in self.rate_limit_counter:\n",
    "                self.rate_limit_counter[user_id] = {current_minute: 1}\n",
    "            elif current_minute in self.rate_limit_counter[user_id]:\n",
    "                self.rate_limit_counter[user_id][current_minute] += 1\n",
    "                if self.rate_limit_counter[user_id][current_minute] > self.max_requests_per_minute:\n",
    "                    raise ValueError(f\"üö® SECURITY ALERT: Rate limit exceeded for user {user_id}\")\n",
    "            else:\n",
    "                self.rate_limit_counter[user_id][current_minute] = 1\n",
    "        \n",
    "        # Convert to numpy array if it's a DataFrame\n",
    "        if hasattr(X_input, 'values'):\n",
    "            X_input_array = X_input.values\n",
    "        else:\n",
    "            X_input_array = X_input\n",
    "            \n",
    "        # Shape validation (basic input sanitization)\n",
    "        if X_input_array.ndim == 1:\n",
    "            X_input_array = X_input_array.reshape(1, -1)\n",
    "        \n",
    "        if X_input_array.shape[1] != self.input_bounds['expected_shape']:\n",
    "            raise ValueError(f\"üö® SECURITY ALERT: Invalid input shape - potential attack detected!\")\n",
    "        \n",
    "        # Adversarial input detection (simplified)\n",
    "        for i in range(X_input_array.shape[1]):\n",
    "            if np.any(X_input_array[:, i] < self.input_bounds['min_values'][i] * 3) or \\\n",
    "               np.any(X_input_array[:, i] > self.input_bounds['max_values'][i] * 3):\n",
    "                warning = f\"‚ö†Ô∏è Suspicious input in feature {self.feature_names[i]} - outside normal range\"\n",
    "                print(warning)\n",
    "                self.suspicious_activity_detected.append(warning)\n",
    "        \n",
    "        return X_input\n",
    "    \n",
    "    def predict(self, X_input, user_id=None, confidence_threshold=0.7):\n",
    "        \"\"\"\n",
    "        Secure prediction with monitoring and validation\n",
    "        \"\"\"\n",
    "        import time\n",
    "        \n",
    "        # Step 1: Validate input (AI security control)\n",
    "        X_validated = self.validate_input(X_input, user_id)\n",
    "        \n",
    "        # Step 2: Apply scaling (convert to numpy array if needed)\n",
    "        if hasattr(X_validated, 'values'):\n",
    "            X_processed = self.scaler.transform(X_validated.values)\n",
    "        else:\n",
    "            X_processed = self.scaler.transform(X_validated)\n",
    "        \n",
    "        # Step 3: Make prediction with confidence monitoring\n",
    "        prediction = self.model.predict(X_processed)\n",
    "        probabilities = self.model.predict_proba(X_processed)\n",
    "        max_confidence = np.max(probabilities, axis=1)\n",
    "        \n",
    "        # Step 4: Log prediction for audit trail\n",
    "        self.prediction_log.append({\n",
    "            'timestamp': time.time(),\n",
    "            'user_id': user_id,\n",
    "            'prediction': prediction.tolist(),\n",
    "            'max_confidence': max_confidence.tolist()\n",
    "        })\n",
    "        \n",
    "        # Step 5: Confidence threshold check\n",
    "        low_confidence_mask = max_confidence < confidence_threshold\n",
    "        if np.any(low_confidence_mask):\n",
    "            print(f\"‚ö†Ô∏è Warning: {np.sum(low_confidence_mask)} low-confidence predictions detected\")\n",
    "        \n",
    "        return prediction, probabilities\n",
    "    \n",
    "    def get_security_report(self):\n",
    "        \"\"\"\n",
    "        Generate security dashboard (monitoring from Chapter 1 framework)\n",
    "        \"\"\"\n",
    "        total_requests = len(self.access_log)\n",
    "        unique_users = len(set([log.get('user_id') for log in self.access_log if log.get('user_id')]))\n",
    "        \n",
    "        report = {\n",
    "            'total_requests': total_requests,\n",
    "            'unique_users': unique_users,\n",
    "            'suspicious_activities': len(self.suspicious_activity_detected),\n",
    "            'total_predictions': len(self.prediction_log)\n",
    "        }\n",
    "        \n",
    "        if self.prediction_log:\n",
    "            confidences = [max(log['max_confidence']) for log in self.prediction_log]\n",
    "            report['avg_confidence'] = np.mean(confidences)\n",
    "            report['low_confidence_predictions'] = sum([1 for c in confidences if c < 0.7])\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Create our secure AI system\n",
    "secure_model = SecureAIModel(\n",
    "    model=model,\n",
    "    scaler=scaler,\n",
    "    feature_names=X.columns.tolist()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Secure AI system deployed with Chapter 1 security controls!\")\n",
    "print(\"üîí Implemented controls: Input validation, Rate limiting, Monitoring, Audit logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6addf9ce",
   "metadata": {},
   "source": [
    "## üß™ Activity 4: Testing Our AI Security (8 minutes)\n",
    "\n",
    "Time for a mini penetration test! Let's simulate the **AI-specific attacks** discussed in Chapter 1.\n",
    "\n",
    "**Key Chapter 1 Concepts Being Tested**:\n",
    "- **Adversarial Examples**: Crafted inputs designed to fool AI\n",
    "- **Model Extraction**: Attempts to steal model functionality\n",
    "- **Infrastructure Attacks**: Overwhelming the system\n",
    "\n",
    "**Your Task**: Act as an attacker and test your security controls!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a460f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Test 1: Normal Operation (Baseline)\n",
    "print(\"üß™ TEST 1: Normal Prediction\")\n",
    "print(\"=\" * 40)\n",
    "normal_sample = X_test.iloc[:3]  # Use first 3 test samples\n",
    "predictions, probabilities = secure_model.predict(normal_sample, user_id=\"legitimate_user\")\n",
    "print(f\"‚úÖ Predictions: {predictions}\")\n",
    "\n",
    "# Fix: Handle array of confidence levels properly\n",
    "confidence_levels = np.max(probabilities, axis=1)\n",
    "confidence_str = \", \".join([f\"{conf:.3f}\" for conf in confidence_levels])\n",
    "print(f\"üìä Confidence levels: [{confidence_str}]\")\n",
    "print(\"Result: Normal operation successful!\\n\")\n",
    "\n",
    "# üß™ Test 2: Adversarial Attack Simulation\n",
    "print(\"üß™ TEST 2: Adversarial Input Attack\")\n",
    "print(\"=\" * 40)\n",
    "adversarial_sample = X_test.iloc[:1].copy()\n",
    "# Simulate adversarial perturbation (extreme values)\n",
    "adversarial_sample.iloc[0, 0] = 100  # Extreme value in first feature\n",
    "print(\"Attempting adversarial attack with extreme input values...\")\n",
    "predictions, probabilities = secure_model.predict(adversarial_sample, user_id=\"attacker\")\n",
    "print(\"Result: Attack detected but prediction still made - need stronger controls!\\n\")\n",
    "\n",
    "# üß™ Test 3: Model Extraction Attack (Rate Limiting)\n",
    "print(\"üß™ TEST 3: Model Extraction Attack (Rate Limiting)\")\n",
    "print(\"=\" * 40)\n",
    "print(\"Simulating rapid queries to extract model behavior...\")\n",
    "try:\n",
    "    # Simulate 105 rapid requests (over our limit of 100)\n",
    "    for i in range(105):\n",
    "        secure_model.predict(X_test.iloc[:1], user_id=\"model_thief\")\n",
    "        if i % 20 == 0:\n",
    "            print(f\"   Query {i+1}/105...\")\n",
    "except ValueError as e:\n",
    "    print(f\"üõ°Ô∏è SECURITY CONTROL TRIGGERED: {e}\")\n",
    "print(\"Result: Rate limiting successfully prevented model extraction!\\n\")\n",
    "\n",
    "# üß™ Test 4: Invalid Input Attack\n",
    "print(\"üß™ TEST 4: Invalid Input Structure Attack\")\n",
    "print(\"=\" * 40)\n",
    "try:\n",
    "    invalid_sample = X_test.iloc[:1, :10]  # Wrong number of features\n",
    "    print(\"Attempting attack with malformed input...\")\n",
    "    secure_model.predict(invalid_sample, user_id=\"structural_attacker\")\n",
    "except ValueError as e:\n",
    "    print(f\"üõ°Ô∏è SECURITY CONTROL TRIGGERED: {e}\")\n",
    "print(\"Result: Input validation successfully blocked malformed attack!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2b0755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Security Dashboard - Monitor Our AI System\n",
    "print(\"üìä AI SECURITY DASHBOARD\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate security report\n",
    "report = secure_model.get_security_report()\n",
    "\n",
    "# Display key metrics\n",
    "print(f\"üîç Security Monitoring Results:\")\n",
    "print(f\"   Total Requests Processed: {report['total_requests']}\")\n",
    "print(f\"   Unique Users: {report['unique_users']}\")\n",
    "print(f\"   Suspicious Activities Detected: {report['suspicious_activities']}\")\n",
    "print(f\"   Total Predictions Made: {report['total_predictions']}\")\n",
    "\n",
    "if 'avg_confidence' in report:\n",
    "    print(f\"   Average Prediction Confidence: {report['avg_confidence']:.1%}\")\n",
    "    print(f\"   Low Confidence Predictions: {report['low_confidence_predictions']}\")\n",
    "\n",
    "# Show detected threats\n",
    "if secure_model.suspicious_activity_detected:\n",
    "    print(f\"\\n‚ö†Ô∏è Threat Detection Log:\")\n",
    "    for i, threat in enumerate(secure_model.suspicious_activity_detected, 1):\n",
    "        print(f\"   {i}. {threat}\")\n",
    "\n",
    "# Security effectiveness analysis\n",
    "print(f\"\\nüõ°Ô∏è Security Control Effectiveness:\")\n",
    "total_attacks = 4  # From our 4 test scenarios\n",
    "blocked_attacks = 2  # Rate limiting + Invalid input\n",
    "effectiveness = (blocked_attacks / total_attacks) * 100\n",
    "print(f\"   Attack Prevention Rate: {effectiveness:.0f}%\")\n",
    "print(f\"   Controls Triggered: {blocked_attacks}/{total_attacks} attack scenarios\")\n",
    "\n",
    "# Recommendations based on Chapter 1 framework\n",
    "print(f\"\\nüìã Recommendations (from Chapter 1 Framework):\")\n",
    "print(f\"   ‚úÖ Implemented: Input validation, Rate limiting, Monitoring\")\n",
    "print(f\"   üîÑ Needs Improvement: Adversarial detection, Confidence thresholds\")\n",
    "print(f\"   ‚û°Ô∏è Next Steps: Implement model encryption, Differential privacy\")\n",
    "\n",
    "print(f\"\\nüéØ Lab Complete! You've experienced AI security in action.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47877863",
   "metadata": {},
   "source": [
    "## ü§î Activity 5: Reflection and Next Steps (5 minutes)\n",
    "\n",
    "Congratulations! You've just built and tested an AI security system. Let's connect this hands-on experience back to the Chapter 1 concepts.\n",
    "\n",
    "**Run the cell below to ake the Chapter 1 Quiz to test your understanding:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eda500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Chapter 1 Quiz in external browser\n",
    "import webbrowser\n",
    "import os\n",
    "\n",
    "# Get the absolute path to the HTML file\n",
    "quiz_path = os.path.join(os.getcwd(), 'chapter1_quiz.html')\n",
    "\n",
    "# Check if file exists\n",
    "if os.path.exists(quiz_path):\n",
    "    print(\"üåê Opening Chapter 1 Quiz in your default browser...\")\n",
    "    webbrowser.open('file://' + quiz_path)\n",
    "    print(\"‚úÖ Quiz opened! Check your browser.\")\n",
    "else:\n",
    "    print(\"‚ùå Quiz file not found. Make sure 'chapter1_quiz.html' exists in the current folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b57f0e",
   "metadata": {},
   "source": [
    "## üéØ Lab Summary: What You Accomplished\n",
    "\n",
    "### ‚úÖ Hands-On Experience Gained\n",
    "\n",
    "**1. Built a Real AI System**: Created a fraud detection model representing production AI systems\n",
    "\n",
    "**2. Experienced AI as Security Target**: Saw how AI systems face unique threats that traditional security can't address\n",
    "\n",
    "**3. Implemented AI Security Framework**: Applied Chapter 1 concepts including:\n",
    "   - Input validation and sanitization\n",
    "   - Rate limiting for model extraction prevention  \n",
    "   - Monitoring and audit logging\n",
    "   - Adversarial input detection\n",
    "\n",
    "**4. Conducted Security Testing**: Performed mini penetration testing against your own AI system\n",
    "\n",
    "**5. Analyzed Security Effectiveness**: Measured and reported on AI-specific security controls\n",
    "\n",
    "### üîë Key Insights from This Lab\n",
    "\n",
    "1. **Traditional Security is Insufficient**: Firewalls and antivirus cannot detect adversarial inputs\n",
    "2. **AI Expands Attack Surface**: Every model feature creates potential attack vectors\n",
    "3. **Defense in Depth Works**: Multiple security layers provide better protection\n",
    "4. **Monitoring is Critical**: AI systems require continuous security monitoring\n",
    "5. **Security Must Be Built-In**: Adding security after deployment is much harder\n",
    "\n",
    "### üöÄ Ready for Chapter 2?\n",
    "\n",
    "You now have practical experience with the AI security fundamentals covered in Chapter 1. In Chapter 2, we'll dive deeper into specific attack techniques and learn advanced defense mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6da2d9",
   "metadata": {},
   "source": [
    "## üí≠ Discussion Questions for Class\n",
    "\n",
    "Based on your hands-on experience, discuss these questions with your classmates:\n",
    "\n",
    "### üè¢ Real-World Application\n",
    "1. **In Your Organization**: What AI systems does your organization use? How are they currently secured?\n",
    "\n",
    "2. **Risk Assessment**: Based on this lab, what would you identify as the highest AI security risks for a financial services company?\n",
    "\n",
    "3. **Implementation Challenges**: What obstacles might prevent organizations from implementing the security controls we demonstrated?\n",
    "\n",
    "### üîç Technical Deep-Dive\n",
    "4. **Attack Sophistication**: Our adversarial attack was simple (extreme values). How might real attackers create more sophisticated adversarial examples?\n",
    "\n",
    "5. **Performance vs Security**: What trade-offs did you notice between security controls and system performance?\n",
    "\n",
    "6. **Monitoring Scale**: How would our monitoring approach need to change for a system processing millions of transactions per day?\n",
    "\n",
    "### üöÄ Future Considerations\n",
    "7. **Emerging Threats**: What new AI security threats might emerge as AI technology advances?\n",
    "\n",
    "8. **Regulatory Impact**: How might emerging AI regulations (like the EU AI Act) affect the security controls we implemented?\n",
    "\n",
    "### üí° Your Next Steps\n",
    "Think about how you'll apply these concepts in your current role or future AI projects. What will you do differently now that you understand AI-specific security requirements?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaae5ff",
   "metadata": {},
   "source": [
    "## üéì Lab Complete - Your AI Security Journey Begins!\n",
    "\n",
    "### üìö Chapter 1 Learning Objectives - ‚úÖ ACHIEVED!\n",
    "\n",
    "- ‚úÖ **Analyzed the critical need** for AI security through hands-on system building\n",
    "- ‚úÖ **Identified key components** of the AI threat landscape via practical attacks  \n",
    "- ‚úÖ **Distinguished between AI as tool vs target** by building and attacking an AI system\n",
    "- ‚úÖ **Evaluated AI security risks** through real testing scenarios\n",
    "- ‚úÖ **Applied fundamental best practices** by implementing security controls\n",
    "- ‚úÖ **Recognized common pitfalls** through failed and successful attack attempts\n",
    "\n",
    "### üõ£Ô∏è Your Path Forward\n",
    "\n",
    "**Next in This Course**:\n",
    "- **Chapter 2**: Advanced threat analysis and sophisticated attack techniques\n",
    "- **Chapter 3**: Building comprehensive AI defense systems  \n",
    "- **Chapter 4**: Adversarial machine learning and countermeasures\n",
    "- **Chapter 5**: AI forensics and incident investigation\n",
    "\n",
    "**In Your Professional Development**:\n",
    "- Start security-first thinking in any AI projects\n",
    "- Advocate for AI security measures in your organization\n",
    "- Join AI security communities and continue learning\n",
    "- Consider AI security certifications and specialization\n",
    "\n",
    "### üî• Keep the Momentum Going!\n",
    "\n",
    "You've taken the first practical step into AI security. The techniques you learned today form the foundation for everything we'll cover in this course. \n",
    "\n",
    "**Remember**: AI security isn't just about protecting technology - it's about protecting the people and organizations that depend on AI systems.\n",
    "\n",
    "---\n",
    "\n",
    "**üöÄ Ready for Chapter 2? Let's dive deeper into the AI threat landscape!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
