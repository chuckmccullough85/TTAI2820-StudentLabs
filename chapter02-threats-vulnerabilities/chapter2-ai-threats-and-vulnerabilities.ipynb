{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b2640c",
   "metadata": {},
   "source": [
    "# Chapter 2: AI Threats and Vulnerabilities - Hands-On Lab\n",
    "\n",
    "Welcome to your practical exploration of AI security threats! This lab will give you hands-on experience with the most common AI vulnerabilities through simple, educational exercises.\n",
    "\n",
    "## üéØ Lab Objectives (45-60 minutes)\n",
    "By completing this lab, you will:\n",
    "- **Understand common AI attack types** through practical demonstrations\n",
    "- **Test AI model vulnerabilities** using simple, effective techniques\n",
    "- **Learn to identify security weaknesses** in machine learning systems\n",
    "- **Apply threat modeling concepts** to real AI scenarios\n",
    "- **Build awareness of AI-specific risks** that traditional security misses\n",
    "\n",
    "## üöÄ What You'll Learn\n",
    "1. **Adversarial Examples** - How small changes can fool AI models\n",
    "2. **Data Poisoning** - How bad training data creates vulnerabilities  \n",
    "3. **Model Extraction** - How attackers can steal AI models\n",
    "4. **Privacy Attacks** - How AI models leak sensitive information\n",
    "5. **Robustness Testing** - How to measure AI system resilience\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of machine learning\n",
    "- Familiarity with Python and data manipulation\n",
    "- Completion of Chapter 1 hands-on lab\n",
    "\n",
    "Let's begin with a simple, practical approach to AI security testing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c6b8e",
   "metadata": {},
   "source": [
    "## üåê Interactive Adversarial Demo - adversarial.js\n",
    "\n",
    "Want to see adversarial attacks in action with a professional, interactive interface? We've cloned the famous **adversarial.js** demonstration by Kenny Song!\n",
    "\n",
    "**What adversarial.js provides:**\n",
    "- **Real-time visualization** of adversarial attacks\n",
    "- **Interactive controls** to adjust attack parameters  \n",
    "- **Live neural network** you can attack in your browser\n",
    "- **Professional interface** perfect for presentations\n",
    "- **Educational explanations** of how attacks work\n",
    "\n",
    "**Perfect for:**\n",
    "- üéì **Instructors**: Live demonstrations during lectures\n",
    "- üë• **Presentations**: Showing stakeholders AI vulnerabilities  \n",
    "- üî¨ **Researchers**: Quick testing and experimentation\n",
    "- üìö **Self-Learning**: Interactive exploration of concepts\n",
    "\n",
    "**üìÅ Location:** The adversarial.js demo is in:\n",
    "```\n",
    "chapter02-threats-vulnerabilities/adversarial.js/\n",
    "```\n",
    "\n",
    "**üöÄ To run the demo:**\n",
    "1. Open a terminal in the adversarial.js directory\n",
    "2. Start a simple HTTP server\n",
    "\n",
    "``` bash\n",
    "python -m http.server 8080\n",
    "```\n",
    "\n",
    "3. Open the demo in your web browser [Click Here for Demo](http://localhost:8080)\n",
    "\n",
    "**Note:** This is the original, high-quality adversarial.js demonstration that's been used by thousands of students and researchers worldwide!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7471686",
   "metadata": {},
   "source": [
    "### üéì What We Learned About Adversarial Attacks\n",
    "\n",
    "**Key Insights:**\n",
    "- **AI models can be fooled** by small changes that humans barely notice\n",
    "- **Neural networks are more vulnerable** than simpler models like logistic regression\n",
    "- **Even small modifications** to images can completely change AI predictions\n",
    "- **These attacks work** even when the AI seems very confident in its wrong answer\n",
    "\n",
    "**Real-World Implications:**\n",
    "- **Self-driving cars** could misidentify stop signs or pedestrians\n",
    "- **Medical AI** could misdiagnose diseases from modified medical images  \n",
    "- **Security systems** could fail to detect threats in manipulated images\n",
    "- **Financial AI** could make wrong decisions based on attacked data\n",
    "\n",
    "**Why This Matters:**\n",
    "- Understanding these vulnerabilities helps us build better defenses\n",
    "- AI security is different from traditional cybersecurity\n",
    "- We need special techniques to protect AI systems\n",
    "\n",
    "**Next:** We'll learn about other types of AI attacks like data poisoning and model theft!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d87e6e3",
   "metadata": {},
   "source": [
    "## üéØ Lab Summary: What You've Accomplished\n",
    "\n",
    "### ‚úÖ Hands-On Experience Gained (For All Backgrounds)\n",
    "\n",
    "**1. üéØ Adversarial Examples** - Discovered how tiny image changes completely fool AI\n",
    "   - **For Security Analysts**: Like sophisticated phishing - small changes, big impact\n",
    "   - **For Data Scientists**: Learned about model brittleness and decision boundaries  \n",
    "   - **For Everyone**: Saw how AI \"sees\" differently than humans\n",
    "\n",
    "### üîë Universal Key Takeaways\n",
    "\n",
    "**1. AI Security ‚â† Traditional Security**\n",
    "- Traditional security focuses on networks, systems, and data\n",
    "- AI security focuses on models, training data, and algorithmic vulnerabilities\n",
    "- Both are needed for comprehensive protection\n",
    "\n",
    "**2. Small Changes, Massive Impact**  \n",
    "- Tiny pixel modifications can completely fool image AI\n",
    "- Small amounts of bad training data create persistent backdoors\n",
    "- Minor noise can cause catastrophic AI failures\n",
    "\n",
    "**3. AI Transparency Creates Vulnerability**\n",
    "- The more attackers know about your AI, the easier it is to attack\n",
    "- Model confidence scores can leak private information\n",
    "- Query access enables model theft and reconnaissance\n",
    "\n",
    "**4. Data is the New Attack Surface**\n",
    "- Training data quality directly affects security\n",
    "- Data provenance and validation are critical\n",
    "- Historical data can contain future vulnerabilities\n",
    "\n",
    "**5. Testing Must Be Proactive**\n",
    "- AI vulnerabilities aren't obvious from normal testing\n",
    "- Security testing requires adversarial thinking\n",
    "- Regular robustness and privacy audits are essential\n",
    "\n",
    "### üõ°Ô∏è Practical Defense Strategies (By Role)\n",
    "\n",
    "**For Security Teams:**\n",
    "- Add AI security to threat modeling processes\n",
    "- Monitor AI system queries for extraction attempts\n",
    "- Include AI components in incident response plans\n",
    "- Validate training data sources and integrity\n",
    "\n",
    "**For Data Science Teams:**\n",
    "- Implement robust data validation pipelines\n",
    "- Use differential privacy techniques when possible\n",
    "- Add adversarial training to improve robustness\n",
    "- Monitor model confidence distributions for privacy leaks\n",
    "\n",
    "**For Management/Decision Makers:**\n",
    "- Budget for AI-specific security tools and training\n",
    "- Include AI security in vendor evaluation criteria\n",
    "- Ensure compliance frameworks cover AI systems\n",
    "- Plan for AI security incident scenarios\n",
    "\n",
    "### üöÄ Ready for Chapter 3: Building AI Defenses\n",
    "\n",
    "You now have practical experience with the major AI attack categories. You understand:\n",
    "- **What** attackers can do to AI systems\n",
    "- **How** these attacks work in practice  \n",
    "- **Why** traditional security isn't enough\n",
    "- **When** AI systems are most vulnerable\n",
    "\n",
    "**Next:** We'll learn how to build comprehensive defenses against these attacks, including:\n",
    "- Adversarial training techniques\n",
    "- Robust model architectures  \n",
    "- Privacy-preserving methods\n",
    "- AI security monitoring systems\n",
    "- Defense-in-depth strategies for AI\n",
    "\n",
    "### üìã Chapter 2 Completion Checklist\n",
    "\n",
    "Before moving to Chapter 3, ensure you can:\n",
    "- [ ] Explain adversarial examples to a non-technical colleague\n",
    "- [ ] Describe how data poisoning creates AI backdoors\n",
    "- [ ] Understand why model extraction is a business risk\n",
    "- [ ] Recognize privacy implications of AI model deployment  \n",
    "- [ ] Appreciate why AI systems need specialized security testing\n",
    "\n",
    "**Congratulations! You're now equipped with fundamental AI security knowledge! üéâ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb00e714",
   "metadata": {},
   "source": [
    "## üìù Chapter 2 Quiz\n",
    "\n",
    "Test your understanding of the AI threats and vulnerabilities covered in this chapter. This interactive quiz will help reinforce key concepts from our hands-on activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b56a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 2 Quiz - Test Your Understanding\n",
    "print(\"üìù Opening Chapter 2 AI Security Quiz...\")\n",
    "\n",
    "import webbrowser\n",
    "import os\n",
    "\n",
    "# Try multiple possible locations for the quiz file\n",
    "quiz_files = [\n",
    "    'chapter2_quiz.html',\n",
    "    '../chapter2_quiz.html', \n",
    "    './chapter02-threats-vulnerabilities/chapter2_quiz.html',\n",
    "    os.path.join(os.getcwd(), 'chapter2_quiz.html')\n",
    "]\n",
    "\n",
    "quiz_opened = False\n",
    "\n",
    "for quiz_path in quiz_files:\n",
    "    if os.path.exists(quiz_path):\n",
    "        print(f\"üåê Opening Chapter 2 Quiz: {quiz_path}\")\n",
    "        try:\n",
    "            webbrowser.open('file://' + os.path.abspath(quiz_path))\n",
    "            print(\"‚úÖ Quiz opened in your default browser!\")\n",
    "            quiz_opened = True\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to open browser: {e}\")\n",
    "\n",
    "if not quiz_opened:\n",
    "    print(\"‚ùå Quiz file not found in expected locations:\")\n",
    "    for path in quiz_files:\n",
    "        print(f\"   ‚Ä¢ {path}\")\n",
    "    print(\"\\nüí° Alternative options:\")\n",
    "    print(\"   1. Ask your instructor for the quiz link\")\n",
    "    print(\"   2. Check the course materials folder\")\n",
    "    print(\"   3. Review the key concepts below instead:\")\n",
    "    \n",
    "    print(\"\\nüéØ KEY CONCEPTS TO REVIEW:\")\n",
    "    print(\"   ‚Ä¢ Adversarial Examples: Small changes that fool AI\")\n",
    "    print(\"   ‚Ä¢ Data Poisoning: Corrupting training data to create backdoors\")\n",
    "    print(\"   ‚Ä¢ Model Extraction: Stealing AI models through queries\")\n",
    "    print(\"   ‚Ä¢ Privacy Attacks: Inferring information about training data\")\n",
    "    print(\"   ‚Ä¢ Robustness Testing: Measuring AI resilience to input changes\")\n",
    "    \n",
    "    print(\"\\nüìö Self-Assessment Questions:\")\n",
    "    print(\"   1. What makes adversarial examples effective against AI?\")\n",
    "    print(\"   2. How much poisoned data is needed to create model backdoors?\") \n",
    "    print(\"   3. What information can membership inference attacks reveal?\")\n",
    "    print(\"   4. Why is robustness testing important for AI security?\")\n",
    "    print(\"   5. What are the main differences between AI and traditional security?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
